{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import csv\n",
    "\n",
    "def get_wikipedia_content(topic):\n",
    "    url = \"https://en.wikipedia.org/w/api.php\"\n",
    "    params = {\n",
    "        \"action\": \"query\",\n",
    "        \"format\": \"json\",\n",
    "        \"titles\": topic,\n",
    "        \"prop\": \"extracts\",\n",
    "        \"explaintext\": True,\n",
    "        \"redirects\": 1\n",
    "    }\n",
    "    \n",
    "    response = requests.get(url, params=params)\n",
    "    data = response.json()\n",
    "    pages = data['query']['pages']\n",
    "    for page_id, page_data in pages.items():\n",
    "        return page_data.get('extract')\n",
    "\n",
    "def generate_topic_data_wikipedia(topics, n_per_topic=2000):\n",
    "    all_data = []\n",
    "    topic_data_count = {}\n",
    "    for topic in topics:\n",
    "        content = get_wikipedia_content(topic)\n",
    "        if content:\n",
    "            sentences = content.split(\". \")\n",
    "            samples = [sentence for sentence in sentences if len(sentence.split()) >= 10]\n",
    "            samples = samples[:n_per_topic]\n",
    "            topic_data_count[topic] = len(samples)\n",
    "            all_data.extend((sentence, topic) for sentence in samples)\n",
    "        else:\n",
    "            print(f\"Failed to retrieve content for {topic}\")\n",
    "            topic_data_count[topic] = 0\n",
    "    return all_data, topic_data_count\n",
    "\n",
    "topics = [\n",
    "\"quantum_mechanics\", \"organic_chemistry\", \"fluid_dynamics\", \"thermodynamics\",\"differential_geometry\",\n",
    "\"plate_tectonics\", \"cellular_respiration\", \"electromagnetic_theory\", \"nuclear_physics\", \"crystal_structure\",\n",
    "\"evolution_theory\", \"chemical_bonding\", \"classical_mechanics\", \"molecular_biology\", \"statistical_mechanics\",\n",
    "\"crystal_structure\", \"quantum_field_theory\", \"particle_physics\", \"astrophysics\", \"cosmology\",   \n",
    "\"evolution_theory\", \"chemical_bonding\", \"classical_mechanics\", \"molecular_biology\", \"statistical_mechanics\",\n",
    "\"classical_mechanics\", \"molecular_biology\", \"statistical_mechanics\"\n",
    "]\n",
    "\n",
    "\n",
    "topic_data, topic_data_count = generate_topic_data_wikipedia(topics, 2000)\n",
    "\n",
    "# CSV 파일로 데이터 저장\n",
    "with open('clear_data.csv', 'w', newline='', encoding='utf-8') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(['text', 'label'])  # 헤더 추가\n",
    "    for text, label in topic_data:\n",
    "        writer.writerow([text, label])\n",
    "\n",
    "print(f\"Total generated data samples: {len(topic_data)}\")\n",
    "print(\"Sample data:\")\n",
    "for i in range(min(5, len(topic_data))):\n",
    "    print(f\"Text: {topic_data[i][0]}\")\n",
    "    print(f\"Label: {topic_data[i][1]}\")\n",
    "    print()\n",
    "\n",
    "print(\"\\nData count per topic:\")\n",
    "for topic, count in topic_data_count.items():\n",
    "    print(f\"{topic}: {count} samples\")\n",
    "\n",
    "print(\"\\nData saved to topic_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from collections import defaultdict\n",
    "\n",
    "# 1. CSV 파일을 읽어서 데이터를 메모리에 로드\n",
    "data_by_topic = defaultdict(list)\n",
    "\n",
    "with open('clear_data.csv', 'r', encoding='utf-8') as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    next(reader)  # 헤더 스킵\n",
    "    for text, label in reader:\n",
    "        data_by_topic[label].append(text)\n",
    "\n",
    "# 2. 토픽별 샘플 수를 계산하고 정렬\n",
    "topic_counts = {topic: len(samples) for topic, samples in data_by_topic.items()}\n",
    "sorted_topics = sorted(topic_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# 3. 상위 15개 토픽 선택\n",
    "top_15_topics = dict(sorted_topics[:15])\n",
    "\n",
    "# 4. 선택된 토픽의 데이터만 새로운 CSV 파일로 저장\n",
    "output_filename = 'f_clear_topic.csv'\n",
    "\n",
    "with open(output_filename, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(['text', 'label'])  # 헤더 추가\n",
    "    \n",
    "    for topic in top_15_topics.keys():\n",
    "        for text in data_by_topic[topic]:\n",
    "            writer.writerow([text, topic])\n",
    "\n",
    "# 5. 결과 출력\n",
    "print(\"전체 토픽 수:\", len(topic_counts))\n",
    "print(\"\\n모든 토픽의 샘플 수 (내림차순):\")\n",
    "for topic, count in sorted_topics:\n",
    "    print(f\"{topic}: {count} samples\")\n",
    "\n",
    "print(\"\\n선택된 상위 15개 토픽:\")\n",
    "for topic, count in top_15_topics.items():\n",
    "    print(f\"{topic}: {count} samples\")\n",
    "\n",
    "print(f\"\\n총 선택된 샘플 수: {sum(top_15_topics.values())}\")\n",
    "print(f\"데이터가 {output_filename}에 저장되었습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "\n",
    "# CSV 파일 읽기\n",
    "df = pd.read_csv('f_clear_topic.csv', delimiter=',')\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # 입력값이 문자열이 아닐 경우 빈 문자열로 처리\n",
    "    if not isinstance(text, str):\n",
    "        return ''\n",
    "    # 소문자 변환\n",
    "    text = text.lower()\n",
    "    # 숫자 및 특수 문자 제거\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)  # 숫자 제거\n",
    "    # 토큰화\n",
    "    tokens = word_tokenize(text)\n",
    "    # 불용어 제거 및 lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [\n",
    "        lemmatizer.lemmatize(word) \n",
    "        for word in tokens \n",
    "        if word not in stop_words and len(word) > 1  # 두 글자 이상만 선택\n",
    "    ]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# 전처리 적용 (text 칼럼에만)\n",
    "df['text'] = df['text'].apply(preprocess_text)\n",
    "\n",
    "# 빈 행 제거\n",
    "df = df[df['text'].str.strip() != '']\n",
    "\n",
    "# 결과 확인\n",
    "print(f\"전처리된 샘플 수: {len(df)}\")\n",
    "print(\"\\n처음 5개 전처리된 샘플:\")\n",
    "print(df.head())\n",
    "\n",
    "# 전처리된 결과를 새 CSV 파일로 저장\n",
    "df.to_csv('p_clear_topic.csv', index=False, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ambiguous Topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import csv\n",
    "\n",
    "def get_wikipedia_content(topic):\n",
    "    url = \"https://en.wikipedia.org/w/api.php\"\n",
    "    params = {\n",
    "        \"action\": \"query\",\n",
    "        \"format\": \"json\",\n",
    "        \"titles\": topic,\n",
    "        \"prop\": \"extracts\",\n",
    "        \"explaintext\": True,\n",
    "        \"redirects\": 1\n",
    "    }\n",
    "    \n",
    "    response = requests.get(url, params=params)\n",
    "    data = response.json()\n",
    "    pages = data['query']['pages']\n",
    "    for page_id, page_data in pages.items():\n",
    "        return page_data.get('extract')\n",
    "\n",
    "def generate_topic_data_wikipedia(topics, n_per_topic=2000):\n",
    "    all_data = []\n",
    "    topic_data_count = {}\n",
    "    for topic in topics:\n",
    "        content = get_wikipedia_content(topic)\n",
    "        if content:\n",
    "            sentences = content.split(\". \")\n",
    "            samples = [sentence for sentence in sentences if len(sentence.split()) >= 10]\n",
    "            samples = samples[:n_per_topic]\n",
    "            topic_data_count[topic] = len(samples)\n",
    "            all_data.extend((sentence, topic) for sentence in samples)\n",
    "        else:\n",
    "            print(f\"Failed to retrieve content for {topic}\")\n",
    "            topic_data_count[topic] = 0\n",
    "    return all_data, topic_data_count\n",
    "\n",
    "topics = [\n",
    "\"machine_learning\", \"deep_learning\", \"natural_language_processing\", \"computer_vision\", \"artificial_intelligence\",\n",
    "\"neural_networks\", \"pattern_recognition\", \"data_mining\", \"big_data_analytics\", \"knowledge_discovery\", \"data_mining\",\n",
    "\"big_data_analytics\", \"knowledge_discovery\", \"cognitive_computing\", \"reinforcement_learning\", \"intelligent_systems\", \"cognitive_computing\", \"reinforcement_learning\", \"intelligent_systems\", \"text_mining\", \"speech_recognition\", \"image_processing\", \"information_retrieval\", \"semantic_analysis\", \"computational_linguistics\", \"sentiment_analysis\", \"speech_recognition\", \"image_processing\", \"information_retrieval\", \"semantic_analysis\", \"computational_linguistics\", \"sentiment_analysis\"\n",
    "]\n",
    "\n",
    "\n",
    "topic_data, topic_data_count = generate_topic_data_wikipedia(topics, 2000)\n",
    "\n",
    "# CSV 파일로 데이터 저장\n",
    "with open('ambi_topic.csv', 'w', newline='', encoding='utf-8') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(['text', 'label'])  # 헤더 추가\n",
    "    for text, label in topic_data:\n",
    "        writer.writerow([text, label])\n",
    "\n",
    "print(f\"Total generated data samples: {len(topic_data)}\")\n",
    "print(\"Sample data:\")\n",
    "for i in range(min(5, len(topic_data))):\n",
    "    print(f\"Text: {topic_data[i][0]}\")\n",
    "    print(f\"Label: {topic_data[i][1]}\")\n",
    "    print()\n",
    "\n",
    "print(\"\\nData count per topic:\")\n",
    "for topic, count in topic_data_count.items():\n",
    "    print(f\"{topic}: {count} samples\")\n",
    "\n",
    "print(\"\\nData saved to topic_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from collections import defaultdict\n",
    "\n",
    "# 1. CSV 파일을 읽어서 데이터를 메모리에 로드\n",
    "data_by_topic = defaultdict(list)\n",
    "\n",
    "with open('ambi_topic.csv', 'r', encoding='utf-8') as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    next(reader)  # 헤더 스킵\n",
    "    for text, label in reader:\n",
    "        data_by_topic[label].append(text)\n",
    "\n",
    "# 2. 토픽별 샘플 수를 계산하고 정렬\n",
    "topic_counts = {topic: len(samples) for topic, samples in data_by_topic.items()}\n",
    "sorted_topics = sorted(topic_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# 3. 상위 15개 토픽 선택\n",
    "top_15_topics = dict(sorted_topics[:15])\n",
    "\n",
    "# 4. 선택된 토픽의 데이터만 새로운 CSV 파일로 저장\n",
    "output_filename = 'f_ambi_topic.csv'\n",
    "\n",
    "with open(output_filename, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(['text', 'label'])  # 헤더 추가\n",
    "    \n",
    "    for topic in top_15_topics.keys():\n",
    "        for text in data_by_topic[topic]:\n",
    "            writer.writerow([text, topic])\n",
    "\n",
    "# 5. 결과 출력\n",
    "print(\"전체 토픽 수:\", len(topic_counts))\n",
    "print(\"\\n모든 토픽의 샘플 수 (내림차순):\")\n",
    "for topic, count in sorted_topics:\n",
    "    print(f\"{topic}: {count} samples\")\n",
    "\n",
    "print(\"\\n선택된 상위 15개 토픽:\")\n",
    "for topic, count in top_15_topics.items():\n",
    "    print(f\"{topic}: {count} samples\")\n",
    "\n",
    "print(f\"\\n총 선택된 샘플 수: {sum(top_15_topics.values())}\")\n",
    "print(f\"데이터가 {output_filename}에 저장되었습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "\n",
    "# CSV 파일 읽기\n",
    "df = pd.read_csv('f_ambi_topic.csv', delimiter=',')\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # 입력값이 문자열이 아닐 경우 빈 문자열로 처리\n",
    "    if not isinstance(text, str):\n",
    "        return ''\n",
    "    # 소문자 변환\n",
    "    text = text.lower()\n",
    "    # 숫자 및 특수 문자 제거\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)  # 숫자 제거\n",
    "    # 토큰화\n",
    "    tokens = word_tokenize(text)\n",
    "    # 불용어 제거 및 lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [\n",
    "        lemmatizer.lemmatize(word) \n",
    "        for word in tokens \n",
    "        if word not in stop_words and len(word) > 1  # 두 글자 이상만 선택\n",
    "    ]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# 전처리 적용 (text 칼럼에만)\n",
    "df['text'] = df['text'].apply(preprocess_text)\n",
    "\n",
    "# 빈 행 제거\n",
    "df = df[df['text'].str.strip() != '']\n",
    "\n",
    "# 결과 확인\n",
    "print(f\"전처리된 샘플 수: {len(df)}\")\n",
    "print(\"\\n처음 5개 전처리된 샘플:\")\n",
    "print(df.head())\n",
    "\n",
    "# 토픽의 갯수 구하기\n",
    "topic_count = df['label'].nunique()\n",
    "\n",
    "print(f\"토픽의 갯수: {topic_count}\")\n",
    "\n",
    "\n",
    "# 전처리된 결과를 새 CSV 파일로 저장\n",
    "df.to_csv('p_ambi_topic.csv', index=False, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More ambiguous Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import csv\n",
    "\n",
    "def get_wikipedia_content(topic):\n",
    "    url = \"https://en.wikipedia.org/w/api.php\"\n",
    "    params = {\n",
    "        \"action\": \"query\",\n",
    "        \"format\": \"json\",\n",
    "        \"titles\": topic,\n",
    "        \"prop\": \"extracts\",\n",
    "        \"explaintext\": True,\n",
    "        \"redirects\": 1\n",
    "    }\n",
    "    \n",
    "    response = requests.get(url, params=params)\n",
    "    data = response.json()\n",
    "    pages = data['query']['pages']\n",
    "    for page_id, page_data in pages.items():\n",
    "        return page_data.get('extract')\n",
    "\n",
    "def generate_topic_data_wikipedia(topics, n_per_topic=2000):\n",
    "    all_data = []\n",
    "    topic_data_count = {}\n",
    "    for topic in topics:\n",
    "        content = get_wikipedia_content(topic)\n",
    "        if content:\n",
    "            sentences = content.split(\". \")\n",
    "            samples = [sentence for sentence in sentences if len(sentence.split()) >= 10]\n",
    "            samples = samples[:n_per_topic]\n",
    "            topic_data_count[topic] = len(samples)\n",
    "            all_data.extend((sentence, topic) for sentence in samples)\n",
    "        else:\n",
    "            print(f\"Failed to retrieve content for {topic}\")\n",
    "            topic_data_count[topic] = 0\n",
    "    return all_data, topic_data_count\n",
    "\n",
    "topics = [\n",
    "    \"Climate Change\", \"Global Warming\", \"Environmental Degradation\", \"Air Pollution\", \"Water Pollution\",\n",
    "    \"Deforestation\", \"Biodiversity Loss\", \"Endangered Species\", \"Renewable Energy\", \"Fossil Fuel Dependency\",\n",
    "    \"Social Inequality\", \"Income Inequality\", \"Gender Inequality\", \"Racial Inequality\", \"Educational Inequality\",\n",
    "    \"Poverty Reduction\", \"Global Poverty\", \"Health Disparities\", \"Access to Healthcare\", \"Food Security\",\n",
    "    \"Food Scarcity\", \"Water Scarcity\", \"Overpopulation\", \"Urbanization Challenges\", \"Housing Crisis\"\n",
    "]\n",
    "\n",
    "\n",
    "topic_data, topic_data_count = generate_topic_data_wikipedia(topics, 2000)\n",
    "\n",
    "# CSV 파일로 데이터 저장\n",
    "with open('moreambi_topic.csv', 'w', newline='', encoding='utf-8') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(['text', 'label'])  # 헤더 추가\n",
    "    for text, label in topic_data:\n",
    "        writer.writerow([text, label])\n",
    "\n",
    "print(f\"Total generated data samples: {len(topic_data)}\")\n",
    "print(\"Sample data:\")\n",
    "for i in range(min(5, len(topic_data))):\n",
    "    print(f\"Text: {topic_data[i][0]}\")\n",
    "    print(f\"Label: {topic_data[i][1]}\")\n",
    "    print()\n",
    "\n",
    "print(\"\\nData count per topic:\")\n",
    "for topic, count in topic_data_count.items():\n",
    "    print(f\"{topic}: {count} samples\")\n",
    "\n",
    "print(\"\\nData saved to topic_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from collections import defaultdict\n",
    "\n",
    "# 1. CSV 파일을 읽어서 데이터를 메모리에 로드\n",
    "data_by_topic = defaultdict(list)\n",
    "\n",
    "with open('moreambi_topic.csv', 'r', encoding='utf-8') as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    next(reader)  # 헤더 스킵\n",
    "    for text, label in reader:\n",
    "        data_by_topic[label].append(text)\n",
    "\n",
    "# 2. 토픽별 샘플 수를 계산하고 정렬\n",
    "topic_counts = {topic: len(samples) for topic, samples in data_by_topic.items()}\n",
    "sorted_topics = sorted(topic_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# 3. 상위 15개 토픽 선택\n",
    "top_15_topics = dict(sorted_topics[:15])\n",
    "\n",
    "# 4. 선택된 토픽의 데이터만 새로운 CSV 파일로 저장\n",
    "output_filename = 'f_moreambi_topic.csv'\n",
    "\n",
    "with open(output_filename, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(['text', 'label'])  # 헤더 추가\n",
    "    \n",
    "    for topic in top_15_topics.keys():\n",
    "        for text in data_by_topic[topic]:\n",
    "            writer.writerow([text, topic])\n",
    "\n",
    "# 5. 결과 출력\n",
    "print(\"전체 토픽 수:\", len(topic_counts))\n",
    "print(\"\\n모든 토픽의 샘플 수 (내림차순):\")\n",
    "for topic, count in sorted_topics:\n",
    "    print(f\"{topic}: {count} samples\")\n",
    "\n",
    "print(\"\\n선택된 상위 15개 토픽:\")\n",
    "for topic, count in top_15_topics.items():\n",
    "    print(f\"{topic}: {count} samples\")\n",
    "\n",
    "print(f\"\\n총 선택된 샘플 수: {sum(top_15_topics.values())}\")\n",
    "print(f\"데이터가 {output_filename}에 저장되었습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "\n",
    "# CSV 파일 읽기\n",
    "df = pd.read_csv('f_moreambi_topic.csv', delimiter=',')\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # 입력값이 문자열이 아닐 경우 빈 문자열로 처리\n",
    "    if not isinstance(text, str):\n",
    "        return ''\n",
    "    # 소문자 변환\n",
    "    text = text.lower()\n",
    "    # 숫자 및 특수 문자 제거\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)  # 숫자 제거\n",
    "    # 토큰화\n",
    "    tokens = word_tokenize(text)\n",
    "    # 불용어 제거 및 lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [\n",
    "        lemmatizer.lemmatize(word) \n",
    "        for word in tokens \n",
    "        if word not in stop_words and len(word) > 1  # 두 글자 이상만 선택\n",
    "    ]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# 전처리 적용 (text 칼럼에만)\n",
    "df['text'] = df['text'].apply(preprocess_text)\n",
    "\n",
    "# 빈 행 제거\n",
    "df = df[df['text'].str.strip() != '']\n",
    "\n",
    "# 결과 확인\n",
    "print(f\"전처리된 샘플 수: {len(df)}\")\n",
    "print(\"\\n처음 5개 전처리된 샘플:\")\n",
    "print(df.head())\n",
    "\n",
    "# 토픽의 갯수 구하기\n",
    "topic_count = df['label'].nunique()\n",
    "\n",
    "print(f\"토픽의 갯수: {topic_count}\")\n",
    "\n",
    "\n",
    "# 전처리된 결과를 새 CSV 파일로 저장\n",
    "df.to_csv('p_moreambi_topic.csv', index=False, header=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "newtopic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
