{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## clear topic "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total generated data samples: 3713\n",
      "Sample data:\n",
      "Text: Quantum mechanics is a fundamental theory that describes the behavior of nature at and below the scale of atoms.: 1.1  It is the foundation of all quantum physics, which includes quantum chemistry, quantum field theory, quantum technology, and quantum information science.\n",
      "Quantum mechanics can describe many systems that classical physics cannot\n",
      "Label: quantum_mechanics\n",
      "\n",
      "Text: Classical physics can describe many aspects of nature at an ordinary (macroscopic and (optical) microscopic) scale, but is not sufficient for describing them at very small submicroscopic (atomic and subatomic) scales\n",
      "Label: quantum_mechanics\n",
      "\n",
      "Text: Most theories in classical physics can be derived from quantum mechanics as an approximation, valid at large (macroscopic/microscopic) scale.\n",
      "Quantum systems have bound states that are quantized to discrete values of energy, momentum, angular momentum, and other quantities, in contrast to classical systems where these quantities can be measured continuously\n",
      "Label: quantum_mechanics\n",
      "\n",
      "Text: Measurements of quantum systems show characteristics of both particles and waves (wave–particle duality), and there are limits to how accurately the value of a physical quantity can be predicted prior to its measurement, given a complete set of initial conditions (the uncertainty principle).\n",
      "Quantum mechanics arose gradually from theories to explain observations that could not be reconciled with classical physics, such as Max Planck's solution in 1900 to the black-body radiation problem, and the correspondence between energy and frequency in Albert Einstein's 1905 paper, which explained the photoelectric effect\n",
      "Label: quantum_mechanics\n",
      "\n",
      "Text: These early attempts to understand microscopic phenomena, now known as the \"old quantum theory\", led to the full development of quantum mechanics in the mid-1920s by Niels Bohr, Erwin Schrödinger, Werner Heisenberg, Max Born, Paul Dirac and others\n",
      "Label: quantum_mechanics\n",
      "\n",
      "\n",
      "Data count per topic:\n",
      "quantum_mechanics: 178 samples\n",
      "organic_chemistry: 126 samples\n",
      "fluid_dynamics: 106 samples\n",
      "thermodynamics: 116 samples\n",
      "differential_geometry: 122 samples\n",
      "plate_tectonics: 233 samples\n",
      "cellular_respiration: 77 samples\n",
      "electromagnetic_theory: 74 samples\n",
      "nuclear_physics: 84 samples\n",
      "crystal_structure: 117 samples\n",
      "evolution_theory: 323 samples\n",
      "chemical_bonding: 124 samples\n",
      "classical_mechanics: 135 samples\n",
      "molecular_biology: 125 samples\n",
      "statistical_mechanics: 84 samples\n",
      "quantum_field_theory: 234 samples\n",
      "particle_physics: 95 samples\n",
      "astrophysics: 57 samples\n",
      "cosmology: 51 samples\n",
      "\n",
      "Data saved to topic_data.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import csv\n",
    "\n",
    "def get_wikipedia_content(topic):\n",
    "    url = \"https://en.wikipedia.org/w/api.php\"\n",
    "    params = {\n",
    "        \"action\": \"query\",\n",
    "        \"format\": \"json\",\n",
    "        \"titles\": topic,\n",
    "        \"prop\": \"extracts\",\n",
    "        \"explaintext\": True,\n",
    "        \"redirects\": 1\n",
    "    }\n",
    "    \n",
    "    response = requests.get(url, params=params)\n",
    "    data = response.json()\n",
    "    pages = data['query']['pages']\n",
    "    for page_id, page_data in pages.items():\n",
    "        return page_data.get('extract')\n",
    "\n",
    "def generate_topic_data_wikipedia(topics, n_per_topic=2000):\n",
    "    all_data = []\n",
    "    topic_data_count = {}\n",
    "    for topic in topics:\n",
    "        content = get_wikipedia_content(topic)\n",
    "        if content:\n",
    "            sentences = content.split(\". \")\n",
    "            samples = [sentence for sentence in sentences if len(sentence.split()) >= 10]\n",
    "            samples = samples[:n_per_topic]\n",
    "            topic_data_count[topic] = len(samples)\n",
    "            all_data.extend((sentence, topic) for sentence in samples)\n",
    "        else:\n",
    "            print(f\"Failed to retrieve content for {topic}\")\n",
    "            topic_data_count[topic] = 0\n",
    "    return all_data, topic_data_count\n",
    "\n",
    "topics = [\n",
    "\"quantum_mechanics\", \"organic_chemistry\", \"fluid_dynamics\", \"thermodynamics\",\"differential_geometry\",\n",
    "\"plate_tectonics\", \"cellular_respiration\", \"electromagnetic_theory\", \"nuclear_physics\", \"crystal_structure\",\n",
    "\"evolution_theory\", \"chemical_bonding\", \"classical_mechanics\", \"molecular_biology\", \"statistical_mechanics\",\n",
    "\"crystal_structure\", \"quantum_field_theory\", \"particle_physics\", \"astrophysics\", \"cosmology\",   \n",
    "\"evolution_theory\", \"chemical_bonding\", \"classical_mechanics\", \"molecular_biology\", \"statistical_mechanics\",\n",
    "\"classical_mechanics\", \"molecular_biology\", \"statistical_mechanics\"\n",
    "]\n",
    "\n",
    "\n",
    "topic_data, topic_data_count = generate_topic_data_wikipedia(topics, 2000)\n",
    "\n",
    "# CSV 파일로 데이터 저장\n",
    "with open('clear_data.csv', 'w', newline='', encoding='utf-8') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(['text', 'label'])  # 헤더 추가\n",
    "    for text, label in topic_data:\n",
    "        writer.writerow([text, label])\n",
    "\n",
    "print(f\"Total generated data samples: {len(topic_data)}\")\n",
    "print(\"Sample data:\")\n",
    "for i in range(min(5, len(topic_data))):\n",
    "    print(f\"Text: {topic_data[i][0]}\")\n",
    "    print(f\"Label: {topic_data[i][1]}\")\n",
    "    print()\n",
    "\n",
    "print(\"\\nData count per topic:\")\n",
    "for topic, count in topic_data_count.items():\n",
    "    print(f\"{topic}: {count} samples\")\n",
    "\n",
    "print(\"\\nData saved to topic_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('ratings_train.txt', <http.client.HTTPMessage at 0x17679c3beb0>)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import urllib.request\n",
    "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/e9t/nsmc/master/ratings_train.txt\", filename=\"ratings_train.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from collections import defaultdict\n",
    "\n",
    "# 1. CSV 파일을 읽어서 데이터를 메모리에 로드\n",
    "data_by_topic = defaultdict(list)\n",
    "\n",
    "with open('clear_data.csv', 'r', encoding='utf-8') as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    next(reader)  # 헤더 스킵\n",
    "    for text, label in reader:\n",
    "        data_by_topic[label].append(text)\n",
    "\n",
    "# 2. 토픽별 샘플 수를 계산하고 정렬\n",
    "topic_counts = {topic: len(samples) for topic, samples in data_by_topic.items()}\n",
    "sorted_topics = sorted(topic_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# 3. 상위 15개 토픽 선택\n",
    "top_15_topics = dict(sorted_topics[:15])\n",
    "\n",
    "# 4. 선택된 토픽의 데이터만 새로운 CSV 파일로 저장\n",
    "output_filename = 'f_clear_topic.csv'\n",
    "\n",
    "with open(output_filename, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(['text', 'label'])  # 헤더 추가\n",
    "    \n",
    "    for topic in top_15_topics.keys():\n",
    "        for text in data_by_topic[topic]:\n",
    "            writer.writerow([text, topic])\n",
    "\n",
    "# 5. 결과 출력\n",
    "print(\"전체 토픽 수:\", len(topic_counts))\n",
    "print(\"\\n모든 토픽의 샘플 수 (내림차순):\")\n",
    "for topic, count in sorted_topics:\n",
    "    print(f\"{topic}: {count} samples\")\n",
    "\n",
    "print(\"\\n선택된 상위 15개 토픽:\")\n",
    "for topic, count in top_15_topics.items():\n",
    "    print(f\"{topic}: {count} samples\")\n",
    "\n",
    "print(f\"\\n총 선택된 샘플 수: {sum(top_15_topics.values())}\")\n",
    "print(f\"데이터가 {output_filename}에 저장되었습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "\n",
    "# CSV 파일 읽기\n",
    "df = pd.read_csv('f_clear_topic.csv', delimiter=',')\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # 입력값이 문자열이 아닐 경우 빈 문자열로 처리\n",
    "    if not isinstance(text, str):\n",
    "        return ''\n",
    "    # 소문자 변환\n",
    "    text = text.lower()\n",
    "    # 숫자 및 특수 문자 제거\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)  # 숫자 제거\n",
    "    # 토큰화\n",
    "    tokens = word_tokenize(text)\n",
    "    # 불용어 제거 및 lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [\n",
    "        lemmatizer.lemmatize(word) \n",
    "        for word in tokens \n",
    "        if word not in stop_words and len(word) > 1  # 두 글자 이상만 선택\n",
    "    ]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# 전처리 적용 (text 칼럼에만)\n",
    "df['text'] = df['text'].apply(preprocess_text)\n",
    "\n",
    "# 빈 행 제거\n",
    "df = df[df['text'].str.strip() != '']\n",
    "\n",
    "# 결과 확인\n",
    "print(f\"전처리된 샘플 수: {len(df)}\")\n",
    "print(\"\\n처음 5개 전처리된 샘플:\")\n",
    "print(df.head())\n",
    "\n",
    "# 전처리된 결과를 새 CSV 파일로 저장\n",
    "df.to_csv('p_clear_topic.csv', index=False, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ambiguous Topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import csv\n",
    "\n",
    "def get_wikipedia_content(topic):\n",
    "    url = \"https://en.wikipedia.org/w/api.php\"\n",
    "    params = {\n",
    "        \"action\": \"query\",\n",
    "        \"format\": \"json\",\n",
    "        \"titles\": topic,\n",
    "        \"prop\": \"extracts\",\n",
    "        \"explaintext\": True,\n",
    "        \"redirects\": 1\n",
    "    }\n",
    "    \n",
    "    response = requests.get(url, params=params)\n",
    "    data = response.json()\n",
    "    pages = data['query']['pages']\n",
    "    for page_id, page_data in pages.items():\n",
    "        return page_data.get('extract')\n",
    "\n",
    "def generate_topic_data_wikipedia(topics, n_per_topic=2000):\n",
    "    all_data = []\n",
    "    topic_data_count = {}\n",
    "    for topic in topics:\n",
    "        content = get_wikipedia_content(topic)\n",
    "        if content:\n",
    "            sentences = content.split(\". \")\n",
    "            samples = [sentence for sentence in sentences if len(sentence.split()) >= 10]\n",
    "            samples = samples[:n_per_topic]\n",
    "            topic_data_count[topic] = len(samples)\n",
    "            all_data.extend((sentence, topic) for sentence in samples)\n",
    "        else:\n",
    "            print(f\"Failed to retrieve content for {topic}\")\n",
    "            topic_data_count[topic] = 0\n",
    "    return all_data, topic_data_count\n",
    "\n",
    "topics = [\n",
    "\"machine_learning\", \"deep_learning\", \"natural_language_processing\", \"computer_vision\", \"artificial_intelligence\",\n",
    "\"neural_networks\", \"pattern_recognition\", \"data_mining\", \"big_data_analytics\", \"knowledge_discovery\", \"data_mining\",\n",
    "\"big_data_analytics\", \"knowledge_discovery\", \"cognitive_computing\", \"reinforcement_learning\", \"intelligent_systems\", \"cognitive_computing\", \"reinforcement_learning\", \"intelligent_systems\", \"text_mining\", \"speech_recognition\", \"image_processing\", \"information_retrieval\", \"semantic_analysis\", \"computational_linguistics\", \"sentiment_analysis\", \"speech_recognition\", \"image_processing\", \"information_retrieval\", \"semantic_analysis\", \"computational_linguistics\", \"sentiment_analysis\"\n",
    "]\n",
    "\n",
    "\n",
    "topic_data, topic_data_count = generate_topic_data_wikipedia(topics, 2000)\n",
    "\n",
    "# CSV 파일로 데이터 저장\n",
    "with open('ambi_topic.csv', 'w', newline='', encoding='utf-8') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(['text', 'label'])  # 헤더 추가\n",
    "    for text, label in topic_data:\n",
    "        writer.writerow([text, label])\n",
    "\n",
    "print(f\"Total generated data samples: {len(topic_data)}\")\n",
    "print(\"Sample data:\")\n",
    "for i in range(min(5, len(topic_data))):\n",
    "    print(f\"Text: {topic_data[i][0]}\")\n",
    "    print(f\"Label: {topic_data[i][1]}\")\n",
    "    print()\n",
    "\n",
    "print(\"\\nData count per topic:\")\n",
    "for topic, count in topic_data_count.items():\n",
    "    print(f\"{topic}: {count} samples\")\n",
    "\n",
    "print(\"\\nData saved to topic_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from collections import defaultdict\n",
    "\n",
    "# 1. CSV 파일을 읽어서 데이터를 메모리에 로드\n",
    "data_by_topic = defaultdict(list)\n",
    "\n",
    "with open('ambi_topic.csv', 'r', encoding='utf-8') as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    next(reader)  # 헤더 스킵\n",
    "    for text, label in reader:\n",
    "        data_by_topic[label].append(text)\n",
    "\n",
    "# 2. 토픽별 샘플 수를 계산하고 정렬\n",
    "topic_counts = {topic: len(samples) for topic, samples in data_by_topic.items()}\n",
    "sorted_topics = sorted(topic_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# 3. 상위 15개 토픽 선택\n",
    "top_15_topics = dict(sorted_topics[:15])\n",
    "\n",
    "# 4. 선택된 토픽의 데이터만 새로운 CSV 파일로 저장\n",
    "output_filename = 'f_ambi_topic.csv'\n",
    "\n",
    "with open(output_filename, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(['text', 'label'])  # 헤더 추가\n",
    "    \n",
    "    for topic in top_15_topics.keys():\n",
    "        for text in data_by_topic[topic]:\n",
    "            writer.writerow([text, topic])\n",
    "\n",
    "# 5. 결과 출력\n",
    "print(\"전체 토픽 수:\", len(topic_counts))\n",
    "print(\"\\n모든 토픽의 샘플 수 (내림차순):\")\n",
    "for topic, count in sorted_topics:\n",
    "    print(f\"{topic}: {count} samples\")\n",
    "\n",
    "print(\"\\n선택된 상위 15개 토픽:\")\n",
    "for topic, count in top_15_topics.items():\n",
    "    print(f\"{topic}: {count} samples\")\n",
    "\n",
    "print(f\"\\n총 선택된 샘플 수: {sum(top_15_topics.values())}\")\n",
    "print(f\"데이터가 {output_filename}에 저장되었습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "\n",
    "# CSV 파일 읽기\n",
    "df = pd.read_csv('f_ambi_topic.csv', delimiter=',')\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # 입력값이 문자열이 아닐 경우 빈 문자열로 처리\n",
    "    if not isinstance(text, str):\n",
    "        return ''\n",
    "    # 소문자 변환\n",
    "    text = text.lower()\n",
    "    # 숫자 및 특수 문자 제거\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)  # 숫자 제거\n",
    "    # 토큰화\n",
    "    tokens = word_tokenize(text)\n",
    "    # 불용어 제거 및 lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [\n",
    "        lemmatizer.lemmatize(word) \n",
    "        for word in tokens \n",
    "        if word not in stop_words and len(word) > 1  # 두 글자 이상만 선택\n",
    "    ]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# 전처리 적용 (text 칼럼에만)\n",
    "df['text'] = df['text'].apply(preprocess_text)\n",
    "\n",
    "# 빈 행 제거\n",
    "df = df[df['text'].str.strip() != '']\n",
    "\n",
    "# 결과 확인\n",
    "print(f\"전처리된 샘플 수: {len(df)}\")\n",
    "print(\"\\n처음 5개 전처리된 샘플:\")\n",
    "print(df.head())\n",
    "\n",
    "# 토픽의 갯수 구하기\n",
    "topic_count = df['label'].nunique()\n",
    "\n",
    "print(f\"토픽의 갯수: {topic_count}\")\n",
    "\n",
    "\n",
    "# 전처리된 결과를 새 CSV 파일로 저장\n",
    "df.to_csv('p_ambi_topic.csv', index=False, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More ambiguous Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import csv\n",
    "\n",
    "def get_wikipedia_content(topic):\n",
    "    url = \"https://en.wikipedia.org/w/api.php\"\n",
    "    params = {\n",
    "        \"action\": \"query\",\n",
    "        \"format\": \"json\",\n",
    "        \"titles\": topic,\n",
    "        \"prop\": \"extracts\",\n",
    "        \"explaintext\": True,\n",
    "        \"redirects\": 1\n",
    "    }\n",
    "    \n",
    "    response = requests.get(url, params=params)\n",
    "    data = response.json()\n",
    "    pages = data['query']['pages']\n",
    "    for page_id, page_data in pages.items():\n",
    "        return page_data.get('extract')\n",
    "\n",
    "def generate_topic_data_wikipedia(topics, n_per_topic=2000):\n",
    "    all_data = []\n",
    "    topic_data_count = {}\n",
    "    for topic in topics:\n",
    "        content = get_wikipedia_content(topic)\n",
    "        if content:\n",
    "            sentences = content.split(\". \")\n",
    "            samples = [sentence for sentence in sentences if len(sentence.split()) >= 10]\n",
    "            samples = samples[:n_per_topic]\n",
    "            topic_data_count[topic] = len(samples)\n",
    "            all_data.extend((sentence, topic) for sentence in samples)\n",
    "        else:\n",
    "            print(f\"Failed to retrieve content for {topic}\")\n",
    "            topic_data_count[topic] = 0\n",
    "    return all_data, topic_data_count\n",
    "\n",
    "topics = [\n",
    "    \"Climate Change\", \"Global Warming\", \"Environmental Degradation\", \"Air Pollution\", \"Water Pollution\",\n",
    "    \"Deforestation\", \"Biodiversity Loss\", \"Endangered Species\", \"Renewable Energy\", \"Fossil Fuel Dependency\",\n",
    "    \"Social Inequality\", \"Income Inequality\", \"Gender Inequality\", \"Racial Inequality\", \"Educational Inequality\",\n",
    "    \"Poverty Reduction\", \"Global Poverty\", \"Health Disparities\", \"Access to Healthcare\", \"Food Security\",\n",
    "    \"Food Scarcity\", \"Water Scarcity\", \"Overpopulation\", \"Urbanization Challenges\", \"Housing Crisis\"\n",
    "]\n",
    "\n",
    "\n",
    "topic_data, topic_data_count = generate_topic_data_wikipedia(topics, 2000)\n",
    "\n",
    "# CSV 파일로 데이터 저장\n",
    "with open('moreambi_topic.csv', 'w', newline='', encoding='utf-8') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(['text', 'label'])  # 헤더 추가\n",
    "    for text, label in topic_data:\n",
    "        writer.writerow([text, label])\n",
    "\n",
    "print(f\"Total generated data samples: {len(topic_data)}\")\n",
    "print(\"Sample data:\")\n",
    "for i in range(min(5, len(topic_data))):\n",
    "    print(f\"Text: {topic_data[i][0]}\")\n",
    "    print(f\"Label: {topic_data[i][1]}\")\n",
    "    print()\n",
    "\n",
    "print(\"\\nData count per topic:\")\n",
    "for topic, count in topic_data_count.items():\n",
    "    print(f\"{topic}: {count} samples\")\n",
    "\n",
    "print(\"\\nData saved to topic_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from collections import defaultdict\n",
    "\n",
    "# 1. CSV 파일을 읽어서 데이터를 메모리에 로드\n",
    "data_by_topic = defaultdict(list)\n",
    "\n",
    "with open('moreambi_topic.csv', 'r', encoding='utf-8') as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    next(reader)  # 헤더 스킵\n",
    "    for text, label in reader:\n",
    "        data_by_topic[label].append(text)\n",
    "\n",
    "# 2. 토픽별 샘플 수를 계산하고 정렬\n",
    "topic_counts = {topic: len(samples) for topic, samples in data_by_topic.items()}\n",
    "sorted_topics = sorted(topic_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# 3. 상위 15개 토픽 선택\n",
    "top_15_topics = dict(sorted_topics[:15])\n",
    "\n",
    "# 4. 선택된 토픽의 데이터만 새로운 CSV 파일로 저장\n",
    "output_filename = 'f_moreambi_topic.csv'\n",
    "\n",
    "with open(output_filename, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(['text', 'label'])  # 헤더 추가\n",
    "    \n",
    "    for topic in top_15_topics.keys():\n",
    "        for text in data_by_topic[topic]:\n",
    "            writer.writerow([text, topic])\n",
    "\n",
    "# 5. 결과 출력\n",
    "print(\"전체 토픽 수:\", len(topic_counts))\n",
    "print(\"\\n모든 토픽의 샘플 수 (내림차순):\")\n",
    "for topic, count in sorted_topics:\n",
    "    print(f\"{topic}: {count} samples\")\n",
    "\n",
    "print(\"\\n선택된 상위 15개 토픽:\")\n",
    "for topic, count in top_15_topics.items():\n",
    "    print(f\"{topic}: {count} samples\")\n",
    "\n",
    "print(f\"\\n총 선택된 샘플 수: {sum(top_15_topics.values())}\")\n",
    "print(f\"데이터가 {output_filename}에 저장되었습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "\n",
    "# CSV 파일 읽기\n",
    "df = pd.read_csv('f_moreambi_topic.csv', delimiter=',')\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # 입력값이 문자열이 아닐 경우 빈 문자열로 처리\n",
    "    if not isinstance(text, str):\n",
    "        return ''\n",
    "    # 소문자 변환\n",
    "    text = text.lower()\n",
    "    # 숫자 및 특수 문자 제거\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)  # 숫자 제거\n",
    "    # 토큰화\n",
    "    tokens = word_tokenize(text)\n",
    "    # 불용어 제거 및 lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [\n",
    "        lemmatizer.lemmatize(word) \n",
    "        for word in tokens \n",
    "        if word not in stop_words and len(word) > 1  # 두 글자 이상만 선택\n",
    "    ]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# 전처리 적용 (text 칼럼에만)\n",
    "df['text'] = df['text'].apply(preprocess_text)\n",
    "\n",
    "# 빈 행 제거\n",
    "df = df[df['text'].str.strip() != '']\n",
    "\n",
    "# 결과 확인\n",
    "print(f\"전처리된 샘플 수: {len(df)}\")\n",
    "print(\"\\n처음 5개 전처리된 샘플:\")\n",
    "print(df.head())\n",
    "\n",
    "# 토픽의 갯수 구하기\n",
    "topic_count = df['label'].nunique()\n",
    "\n",
    "print(f\"토픽의 갯수: {topic_count}\")\n",
    "\n",
    "\n",
    "# 전처리된 결과를 새 CSV 파일로 저장\n",
    "df.to_csv('p_moreambi_topic.csv', index=False, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 통계"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 문서 수: 3444\n",
      "평균 단어 수: 21.48199767711963\n",
      "평균 단어 길이: 7.066325047275514\n",
      "상위 10개 단어 빈도: [('data', 1606), ('learning', 762), ('system', 512), ('model', 497), ('speech', 460), ('image', 449), ('recognition', 435), ('information', 427), ('big', 425), ('used', 410)]\n",
      "토픽별 문서 수:\n",
      " label\n",
      "big_data_analytics             506\n",
      "speech_recognition             480\n",
      "artificial_intelligence        365\n",
      "sentiment_analysis             282\n",
      "reinforcement_learning         268\n",
      "deep_learning                  251\n",
      "machine_learning               235\n",
      "data_mining                    156\n",
      "image_processing               152\n",
      "computer_vision                149\n",
      "knowledge_discovery            144\n",
      "information_retrieval          136\n",
      "natural_language_processing    112\n",
      "pattern_recognition             85\n",
      "text_mining                     67\n",
      "cognitive_computing             56\n",
      "Name: count, dtype: int64\n",
      "\n",
      "처음 5개 데이터:\n",
      "                                                text               label  \\\n",
      "0  big data primarily refers data set large compl...  big_data_analytics   \n",
      "1  data many entry row offer greater statistical ...  big_data_analytics   \n",
      "2  though used sometimes loosely partly due lack ...  big_data_analytics   \n",
      "3  big data originally associated three key conce...  big_data_analytics   \n",
      "4  analysis big data present challenge sampling t...  big_data_analytics   \n",
      "\n",
      "   word_count  avg_word_length  \n",
      "0          12         6.583333  \n",
      "1          19         5.789474  \n",
      "2          39         6.794872  \n",
      "3          10         6.300000  \n",
      "4          11         7.272727  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 저장된 파일 불러오기\n",
    "df = pd.read_csv('more_similar_topic.csv', delimiter=',')\n",
    "\n",
    "\n",
    "# 필요한 라이브러리 임포트\n",
    "from collections import Counter\n",
    "\n",
    "# 텍스트 길이(단어 수) 및 평균 단어 길이 통계 추가\n",
    "df['word_count'] = df['text'].apply(lambda x: len(x.split()))\n",
    "df['avg_word_length'] = df['text'].apply(lambda x: np.mean([len(word) for word in x.split()]) if x else 0)\n",
    "\n",
    "# 전체 단어 빈도 계산\n",
    "all_words = ' '.join(df['text']).split()\n",
    "word_freq = Counter(all_words)\n",
    "\n",
    "# 토픽별 문서 수 계산\n",
    "topic_counts = df['label'].value_counts()\n",
    "\n",
    "# 통계 출력\n",
    "print(\"총 문서 수:\", len(df))\n",
    "print(\"평균 단어 수:\", df['word_count'].mean())\n",
    "print(\"평균 단어 길이:\", df['avg_word_length'].mean())\n",
    "print(\"상위 10개 단어 빈도:\", word_freq.most_common(10))\n",
    "print(\"토픽별 문서 수:\\n\", topic_counts)\n",
    "\n",
    "# 통계 요약을 DataFrame으로 정리하여 저장\n",
    "stats_df = pd.DataFrame({\n",
    "    'Total Documents': [len(df)],\n",
    "    'Average Word Count': [df['word_count'].mean()],\n",
    "    'Average Word Length': [df['avg_word_length'].mean()],\n",
    "    'Top 10 Words': [word_freq.most_common(10)],\n",
    "    'Topic Counts': [topic_counts.to_dict()]\n",
    "})\n",
    "\n",
    "# # CSV 파일로 저장\n",
    "# stats_df.to_csv('text_data_statistics.csv', index=False)\n",
    "\n",
    "# 상위 5개 데이터 출력\n",
    "print(\"\\n처음 5개 데이터:\")\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# CSV 파일 읽기\n",
    "df = pd.read_csv('pre_data.csv', delimiter=',')\n",
    "\n",
    "# 전체 샘플 수와 목표 샘플 수 설정\n",
    "total_samples = len(df)\n",
    "target_samples = 1000\n",
    "\n",
    "# 각 카테고리별 비율 계산\n",
    "category_ratios = df['label'].value_counts(normalize=True)\n",
    "\n",
    "# 각 카테고리별 샘플 수 계산 (비율 유지)\n",
    "category_samples = (category_ratios * target_samples).round().astype(int)\n",
    "\n",
    "# 반올림으로 인한 오차 보정\n",
    "diff = target_samples - category_samples.sum()\n",
    "if diff != 0:\n",
    "    category_samples[category_samples.index[0]] += diff\n",
    "\n",
    "# 층화 샘플링 수행\n",
    "sampled_df = pd.DataFrame()\n",
    "for category, n_samples in category_samples.items():\n",
    "    category_df = df[df['label'] == category]\n",
    "    sampled_category = category_df.sample(n=n_samples, random_state=42)\n",
    "    sampled_df = pd.concat([sampled_df, sampled_category])\n",
    "\n",
    "# 결과 확인\n",
    "print(f\"원본 데이터 크기: {len(df)}\")\n",
    "print(f\"샘플링된 데이터 크기: {len(sampled_df)}\")\n",
    "print(\"\\n원본 데이터의 카테고리 분포:\")\n",
    "print(df['label'].value_counts())\n",
    "print(\"\\n샘플링된 데이터의 카테고리 분포:\")\n",
    "print(sampled_df['label'].value_counts())\n",
    "\n",
    "# 샘플링된 데이터를 새로운 CSV 파일로 저장\n",
    "sampled_df.to_csv('sampled_topic_data.csv', index=False)\n",
    "print(\"\\n샘플링된 데이터가 'sampled_topic_data.csv'로 저장되었습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import seaborn as sns\n",
    "\n",
    "# CSV 파일 읽기\n",
    "df = pd.read_csv('pre_data.csv')\n",
    "\n",
    "# 기본 정보 출력\n",
    "print(f\"Total number of documents: {len(df)}\")\n",
    "print(f\"Number of unique topics: {df['label'].nunique()}\")\n",
    "\n",
    "# 토픽별 문서 수\n",
    "topic_counts = df['label'].value_counts()\n",
    "print(\"\\nNumber of documents per topic:\")\n",
    "print(topic_counts)\n",
    "\n",
    "# 문장 길이 계산\n",
    "df['sentence_length'] = df['text'].apply(lambda x: len(x.split()))\n",
    "\n",
    "# 기초 통계 계산\n",
    "print(\"\\nBasic statistics of sentence length:\")\n",
    "print(df['sentence_length'].describe())\n",
    "\n",
    "# 토픽별 평균 문장 길이\n",
    "avg_sentence_length = df.groupby('label')['sentence_length'].mean().sort_values(ascending=False)\n",
    "print(\"\\nAverage sentence length per topic:\")\n",
    "print(avg_sentence_length)\n",
    "\n",
    "# 가장 많이 사용된 단어 (불용어 제외)\n",
    "def get_top_words(text, n=10):\n",
    "    words = text.lower().split()\n",
    "    word_counts = Counter(words)\n",
    "    return word_counts.most_common(n)\n",
    "\n",
    "top_words = get_top_words(' '.join(df['text']))\n",
    "print(\"\\nTop 10 most common words:\")\n",
    "print(top_words)\n",
    "\n",
    "# 시각화: 토픽별 문서 수\n",
    "plt.figure(figsize=(12, 6))\n",
    "topic_counts.plot(kind='bar')\n",
    "plt.title('Number of Documents per Topic')\n",
    "plt.xlabel('Topic')\n",
    "plt.ylabel('Number of Documents')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "\n",
    "# 시각화: 문장 길이 분포\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.histplot(df['sentence_length'], kde=True)\n",
    "plt.title('Distribution of Sentence Lengths')\n",
    "plt.xlabel('Sentence Length (words)')\n",
    "plt.ylabel('Frequency')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the preprocessed data\n",
    "df = pd.read_csv('pre_data.csv')\n",
    "\n",
    "# Get the unique labels and their counts\n",
    "label_counts = df['label'].value_counts()\n",
    "\n",
    "# Print the label names and their counts\n",
    "print(\"Label counts:\")\n",
    "for label, count in label_counts.items():\n",
    "    print(f\"{label}: {count} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전처리된 데이터가 p_text.csv에 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from konlpy.tag import Okt\n",
    "\n",
    "# 데이터 파일 경로\n",
    "file_path = \"text.csv\"\n",
    "\n",
    "# 데이터 로드\n",
    "data = pd.read_csv(file_path, sep=',')\n",
    "\n",
    "# 불용어 목록 (예시)\n",
    "stopwords = ['의', '가', '이', '은', '는', '에', '과', '도', '를', '을', '에서', '와', '한', '하다']\n",
    "\n",
    "# 전처리 함수\n",
    "def preprocess_text(text):\n",
    "    # 특수 문자 제거\n",
    "    text = re.sub(r'[^가-힣\\s]', '', text)  # 한글과 공백만 남김\n",
    "    # 형태소 분석\n",
    "    okt = Okt()\n",
    "    tokens = okt.morphs(text)  # 형태소 단위로 분리\n",
    "    # 불용어 제거\n",
    "    tokens = [word for word in tokens if word not in stopwords]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# 'document' 컬럼에 대해 전처리 수행 (컬럼 이름을 실제 데이터에 맞게 수정하세요)\n",
    "data['processed_document'] = data['document'].apply(preprocess_text)\n",
    "\n",
    "# 전처리된 데이터를 새로운 CSV 파일로 저장\n",
    "data['processed_document'].to_csv('p_text.csv', index=False)\n",
    "\n",
    "print(\"전처리된 데이터가 p_text.csv에 저장되었습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# CSV 파일 경로 (여기서 'iris.txt'는 실제 경로로 수정해야 합니다)\n",
    "file_path = 'iris.txt'  # 파일 이름은 'iris.txt'로 수정\n",
    "\n",
    "# 데이터셋 불러오기\n",
    "iris_df = pd.read_csv(file_path, header=None, names=['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'species'])\n",
    "\n",
    "# 데이터셋을 새로운 CSV 파일로 저장\n",
    "iris_df.to_csv('iris_output.txt', index=False)  # 파일 이름을 'iris_output.csv'로 수정"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "newtopic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
