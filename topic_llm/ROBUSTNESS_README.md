# LLM Robustness Analysis Framework

3ê°œì˜ ë…ë¦½ì ì¸ ë¶„ì„ ìŠ¤í¬ë¦½íŠ¸ë¡œ êµ¬ì„±ëœ LLM í‰ê°€ ê°•ê±´ì„± ê²€ì¦ í”„ë ˆì„ì›Œí¬

---

## ğŸ“ íŒŒì¼ êµ¬ì¡°

```
topic_llm/
â”œâ”€â”€ AI_eval.py                      # ë©”ì¸ í‰ê°€ ìŠ¤í¬ë¦½íŠ¸ (ì´ë¯¸ ì¡´ì¬)
â”œâ”€â”€ llm_metadata_logger.py          # ë©”íƒ€ë°ì´í„° ìˆ˜ì§‘ ë° ë¡œê¹…
â”œâ”€â”€ cohens_kappa_analysis.py        # Cohen's Îº ê³„ì‚° (inter-rater reliability)
â”œâ”€â”€ llm_robustness_analysis.py      # ê°•ê±´ì„± í…ŒìŠ¤íŠ¸ (temperature, prompt, multi-LLM)
â””â”€â”€ ROBUSTNESS_README.md            # ì´ íŒŒì¼
```

---

## 1. LLM Metadata Logger

### ëª©ì 
ëª¨ë“  LLM API í˜¸ì¶œì˜ ë©”íƒ€ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•˜ê³  ë¬¸ì„œí™”í•©ë‹ˆë‹¤.

### ì£¼ìš” ê¸°ëŠ¥
- âœ… ëª¨ë¸ëª…/ë²„ì „ ê¸°ë¡
- âœ… API íŒŒë¼ë¯¸í„° (temperature, max_tokens, top_p) ê¸°ë¡
- âœ… í˜¸ì¶œ íƒ€ì„ìŠ¤íƒ¬í”„
- âœ… í‰ê°€ ì„¤ê³„ íŒŒë¼ë¯¸í„° (datasets, metrics, aggregation method)
- âœ… Deterministic sampling ì—¬ë¶€ (temperature=0)
- âœ… JSON í˜•ì‹ ì €ì¥ (ì¬ë¶„ì„ ê°€ëŠ¥)

### ì‚¬ìš© ë°©ë²•

#### ê¸°ë³¸ ì‚¬ìš© ì˜ˆì œ
```python
from topic_llm.llm_metadata_logger import LLMMetadataLogger

# ë¡œê±° ìƒì„±
logger = LLMMetadataLogger(output_dir="llm_evaluation_logs")

# í‰ê°€ ì„¤ê³„ íŒŒë¼ë¯¸í„° ì„¤ì •
logger.set_evaluation_parameters(
    datasets=["distinct", "similar", "more_similar"],
    num_topics_per_dataset={"distinct": 15, "similar": 15, "more_similar": 16},
    metrics_evaluated=["coherence", "distinctiveness", "diversity", "semantic_integration"],
    aggregation_method="mean",
    num_iterations=1
)

# API í˜¸ì¶œ ë¡œê¹…
logger.log_api_call(
    evaluator_name="AnthropicEval",
    model_name="claude-sonnet-4-5-20250929",
    metric="coherence_batch",
    prompt="Rate the coherence...",
    response="<topic1>0.92</topic1>...",
    api_parameters={
        "temperature": 0,
        "max_tokens": 1024,
        "top_p": None,
    },
    dataset_name="distinct",
    topic_indices=list(range(15))
)

# ì„¸ì…˜ ì¢…ë£Œ ë° ì €ì¥
metadata_path = logger.finalize_session()

# ë¦¬í¬íŠ¸ ìƒì„±
from topic_llm.llm_metadata_logger import create_metadata_report
report = create_metadata_report(metadata_path)
print(report)
```

#### AI_eval.pyì— í†µí•©í•˜ê¸°

```python
# AI_eval.py ìƒë‹¨ì— ì¶”ê°€
from topic_llm.llm_metadata_logger import LLMMetadataLogger

def run_llm_evaluation() -> pd.DataFrame:
    # ë¡œê±° ì´ˆê¸°í™”
    logger = LLMMetadataLogger()
    logger.set_evaluation_parameters(
        datasets=["distinct", "similar", "more_similar"],
        num_topics_per_dataset={"distinct": 15, "similar": 15, "more_similar": 16},
        metrics_evaluated=["coherence", "distinctiveness", "diversity", "semantic_integration"],
        aggregation_method="mean",
        num_iterations=1
    )

    # ê¸°ì¡´ í‰ê°€ ì½”ë“œ...

    # ì„¸ì…˜ ì¢…ë£Œ
    logger.finalize_session()
```

### ì¶œë ¥ ì˜ˆì‹œ

**Metadata JSON íŒŒì¼** (`metadata_20250117_120000.json`):
```json
{
  "session_id": "20250117_120000",
  "start_time": "2025-01-17T12:00:00",
  "total_api_calls": 12,
  "deterministic": true,
  "aggregation_method": "mean",
  "api_calls": [
    {
      "timestamp": "2025-01-17T12:00:05",
      "evaluator": "AnthropicEval",
      "model": "claude-sonnet-4-5-20250929",
      "metric": "coherence_batch",
      "dataset": "distinct",
      "api_parameters": {
        "temperature": 0,
        "max_tokens": 1024
      }
    }
  ]
}
```

**Report Markdown**:
```markdown
# LLM Evaluation Metadata Report

**Session ID**: 20250117_120000
**Date**: 2025-01-17T12:00:00
**Total API Calls**: 12

## API Configuration

- **Model**: claude-sonnet-4-5-20250929
- **Temperature**: 0
- **Max Tokens**: 1024
- **Deterministic**: Yes (temperature=0)

## Evaluation Design

- **Datasets**: distinct, similar, more_similar
- **Metrics**: coherence, distinctiveness, diversity, semantic_integration
- **Aggregation Method**: mean
- **Iterations per Item**: 1
```

---

## 2. Cohen's Kappa Analysis

### ëª©ì 
ì—°ì†í˜• LLM ì ìˆ˜ë¥¼ ë²”ì£¼í˜• ë ˆì´ë¸”ë¡œ ë³€í™˜í•˜ê³  Cohen's Îºë¥¼ ê³„ì‚°í•˜ì—¬ inter-rater reliabilityë¥¼ ì¸¡ì •í•©ë‹ˆë‹¤.

### ì£¼ìš” ê¸°ëŠ¥
- âœ… ì—°ì†í˜• â†’ ë²”ì£¼í˜• ë³€í™˜ (customizable thresholds)
- âœ… Cohen's Îº ê³„ì‚° (unweighted, linear, quadratic)
- âœ… Confusion matrix ìƒì„±
- âœ… Multi-rater analysis (3+ LLMs)
- âœ… ì˜ì‚¬ì½”ë“œ + ì‹¤ì œ êµ¬í˜„ í¬í•¨

### ì‚¬ìš© ë°©ë²•

#### ë‹¨ì¼ ë¶„ì„
```python
from topic_llm.cohens_kappa_analysis import analyze_llm_agreement
import numpy as np

# LLM ì ìˆ˜ ì¤€ë¹„ (ì˜ˆ: Anthropic vs OpenAI)
scores_anthropic = np.array([0.92, 0.85, 0.88, ...])  # 45ê°œ topics
scores_openai = np.array([0.89, 0.82, 0.91, ...])

# ë¶„ì„ ì‹¤í–‰
results = analyze_llm_agreement(
    scores_anthropic,
    scores_openai,
    thresholds=[0.60, 0.80],  # 3 categories: poor(<0.6), acceptable(0.6-0.8), excellent(>0.8)
    labels=['poor', 'acceptable', 'excellent'],
    weights=None  # Unweighted (nominal categories)
)

# ë¦¬í¬íŠ¸ ìƒì„±
from topic_llm.cohens_kappa_analysis import generate_kappa_report
report = generate_kappa_report(results)
print(report)
```

#### Multi-rater ë¶„ì„
```python
from topic_llm.cohens_kappa_analysis import multi_rater_analysis

# 3ê°œ LLM ì ìˆ˜
scores_dict = {
    "Anthropic": scores_anthropic,
    "OpenAI": scores_openai,
    "Gemini": scores_gemini,
}

# Pairwise kappa ê³„ì‚°
kappa_matrix = multi_rater_analysis(scores_dict, thresholds=[0.60, 0.80])
print(kappa_matrix)
```

### ì¶œë ¥ ì˜ˆì‹œ

```
# Cohen's Kappa Analysis Report

## Inter-Rater Reliability

**Cohen's Îº**: 0.742
**Interpretation**: Substantial agreement

### Agreement Statistics

- Observed Agreement (p_o): 0.822
- Expected Agreement (p_e): 0.289
- Number of Samples: 45

### Confusion Matrix

```
         poor  acceptable  excellent
poor        5           2          0
acceptable  1          18          3
excellent   0           2         14
```

## Continuous Score Analysis

- Pearson Correlation: 0.865
- Mean Absolute Difference: 0.042

### Score Distributions

- LLM 1: Î¼=0.857, Ïƒ=0.068
- LLM 2: Î¼=0.840, Ïƒ=0.095
```

### Cohen's Îº í•´ì„ (Landis & Koch, 1977)
- **< 0.00**: Poor (randomë³´ë‹¤ ë‚˜ì¨)
- **0.00 - 0.20**: Slight agreement
- **0.21 - 0.40**: Fair agreement
- **0.41 - 0.60**: Moderate agreement
- **0.61 - 0.80**: Substantial agreement
- **0.81 - 1.00**: Almost perfect agreement

---

## 3. LLM Robustness Analysis

### ëª©ì 
Temperature, prompt variation, multi-LLM ë“± ë‹¤ì–‘í•œ ì¡°ê±´ì—ì„œ LLM í‰ê°€ì˜ ê°•ê±´ì„±ì„ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤.

### ì£¼ìš” ê¸°ëŠ¥
- âœ… Temperature sensitivity analysis (0.0, 0.3, 0.7, 1.0)
- âœ… Prompt variation testing (5 semantic variations)
- âœ… Multi-LLM comparison (Anthropic, OpenAI, Gemini)
- âœ… Multi-iteration stability (5-10 runs)
- âœ… Statistical analysis (std, CV, ANOVA)
- âœ… Mitigation strategy recommendations

### âš ï¸ ì£¼ì˜ì‚¬í•­

**API í˜¸ì¶œ ë¹„ìš©ì´ ë†’ìŠµë‹ˆë‹¤!**
- Temperature sweep: 4 Ã— 12 = 48 calls
- Prompt variations: 5 Ã— 12 = 60 calls
- Multi-LLM: 3 Ã— 12 = 36 calls
- Multi-iteration: 5 Ã— 12 = 60 calls
- **Total: ~200-300 API calls**
- **Estimated cost: $0.60 - $4.50**

### ì‚¬ìš© ë°©ë²•

#### ì „ì²´ ë¶„ì„ (ì£¼ì˜: ë¹„ìš© ë°œìƒ)
```python
from topic_llm.llm_robustness_analysis import RobustnessAnalyzer
from topic_llm.anthropic_topic_evaluator import TopicEvaluatorLLM as AnthropicEval

# Analyzer ì´ˆê¸°í™”
analyzer = RobustnessAnalyzer(output_dir="robustness_results")

# 1. Temperature sensitivity
analyzer.temperature_sensitivity_analysis(
    AnthropicEval,
    temperatures=[0.0, 0.3, 0.7, 1.0],
    dataset="distinct"
)

# 2. Prompt variations
analyzer.prompt_variation_analysis(
    AnthropicEval,
    dataset="distinct",
    num_variations=5
)

# 3. Multi-LLM comparison
from topic_llm.openai_topic_evaluator import TopicEvaluatorLLM as OpenAIEval
from topic_llm.gemini_topic_evaluator import TopicEvaluatorLLM as GeminiEval

analyzer.multi_llm_comparison(
    [AnthropicEval, OpenAIEval, GeminiEval],
    ["Anthropic", "OpenAI", "Gemini"],
    dataset="distinct"
)

# 4. Multi-iteration stability
analyzer.multi_iteration_stability(
    AnthropicEval,
    dataset="distinct",
    num_iterations=5
)

# 5. Mitigation strategies
analyzer.propose_mitigation_strategies()

# Generate report
report = analyzer.generate_report(output_path="robustness_report.md")
print(report)

# Save results
analyzer.save_results("robustness_results.pkl")
```

#### ë‹¨ê³„ë³„ í…ŒìŠ¤íŠ¸ (ê¶Œì¥)
```python
# ë¨¼ì € ë‹¨ì¼ datasetìœ¼ë¡œ í…ŒìŠ¤íŠ¸
analyzer = RobustnessAnalyzer()

# 1. Temperatureë§Œ í…ŒìŠ¤íŠ¸ (12 API calls)
temp_results = analyzer.temperature_sensitivity_analysis(
    AnthropicEval,
    temperatures=[0.0, 0.3, 0.7, 1.0],
    dataset="distinct"
)

# ê²°ê³¼ í™•ì¸
print(f"Mean range: {temp_results['statistics']['mean_range']:.3f}")
print(f"Mean CV: {temp_results['statistics']['mean_cv']:.3f}")

# 2. ê²°ê³¼ê°€ ë§Œì¡±ìŠ¤ëŸ¬ìš°ë©´ ì „ì²´ ë¶„ì„ ì‹¤í–‰
# ...
```

### ì¶œë ¥ ì˜ˆì‹œ

```markdown
# LLM Robustness Analysis Report

**Generated**: 2025-01-17 14:30:00

## 1. Temperature Sensitivity Analysis

**Temperatures tested**: [0.0, 0.3, 0.7, 1.0]

### Results

| Temperature | Mean Score | Std Dev | CV |
|-------------|------------|---------|------|
| 0.0 | 0.887 | 0.068 | 0.077 |
| 0.3 | 0.891 | 0.072 | 0.081 |
| 0.7 | 0.875 | 0.095 | 0.109 |
| 1.0 | 0.862 | 0.118 | 0.137 |

**Mean Range**: 0.029 (low sensitivity âœ“)
**Mean CV**: 0.033

## 2. Prompt Variation Analysis

**Variations tested**: 5

| Variation | Mean Score | Std Dev | CV |
|-----------|------------|---------|------|
| variation_1 | 0.887 | 0.068 | 0.077 |
| variation_2 | 0.892 | 0.065 | 0.073 |
| variation_3 | 0.883 | 0.071 | 0.080 |
| variation_4 | 0.889 | 0.069 | 0.078 |
| variation_5 | 0.885 | 0.070 | 0.079 |

**Mean Range**: 0.009 (very stable âœ“)
**Mean CV**: 0.010

## 3. Multi-LLM Comparison

**Models tested**: Anthropic, OpenAI, Gemini

### Inter-Model Agreement

- **Mean Pairwise Correlation**: 0.823 (high âœ“)
- **Mean Topic Variance**: 0.005 (low âœ“)

## 4. Mitigation Strategies

### Temperature

- **Recommendation**: Temperature-insensitive (robust)
- **Suggested Value**: 0.0
- **Rationale**: Minimal variation across temperatures, use deterministic (T=0)

### Multi-Model Consensus

- **Recommendation**: High inter-model agreement
- **Method**: Use any single model (cost-effective)
- **Rationale**: Strong correlation (r=0.823) between models

### Ensemble Methods (Recommended)

- Temperature ensemble: Average scores from Tâˆˆ{0, 0.3, 0.7}
- Model ensemble: Average scores from Anthropic + OpenAI + Gemini
- Prompt ensemble: Average scores from 3-5 prompt variations
- Iteration ensemble: Average scores from 5+ independent runs

**Expected Improvement**: 15-30% reduction in score variance

## Conclusions

1. **Bias Mitigation**: Use ensemble methods to reduce single-model bias
2. **Hallucination Risk**: Temperature=0 minimizes hallucination
3. **Score Stability**: Multi-iteration averaging improves reliability
4. **Best Practice**: Combine 2+ LLMs with prompt ensemble for critical evaluations
```

---

## ğŸ“Š ì‹¤í–‰ ì›Œí¬í”Œë¡œìš°

### ë…¼ë¬¸ì„ ìœ„í•œ ê¶Œì¥ ì›Œí¬í”Œë¡œìš°

```
1. AI_eval.py ì‹¤í–‰ (baseline ê²°ê³¼)
   â””â”€> 12 API calls per LLM
   â””â”€> Output: baseline scores

2. llm_metadata_logger.py í†µí•©
   â””â”€> AI_eval.pyì— ë¡œê¹… ì¶”ê°€
   â””â”€> Output: metadata JSON + report

3. cohens_kappa_analysis.py ì‹¤í–‰
   â””â”€> 2+ LLM ê²°ê³¼ë¡œ Cohen's Îº ê³„ì‚°
   â””â”€> Output: inter-rater reliability report

4. llm_robustness_analysis.py ì‹¤í–‰ (ë‹¨ê³„ë³„)
   4a. Temperature sensitivity (12 calls)
       â””â”€> Temperature=0 ì‚¬ìš© ì •ë‹¹í™”

   4b. Prompt variations (60 calls, ì„ íƒì‚¬í•­)
       â””â”€> Prompt stability ê²€ì¦

   4c. Multi-LLM comparison (36 calls)
       â””â”€> Model agreement ê²€ì¦

   4d. Multi-iteration stability (60 calls, ì„ íƒì‚¬í•­)
       â””â”€> Score stability ê²€ì¦

5. Mitigation strategies ë¬¸ì„œí™”
   â””â”€> ìµœì¢… ë…¼ë¬¸ì— í¬í•¨
```

### ìµœì†Œ ìš”êµ¬ì‚¬í•­ (ë¹„ìš© ì ˆê°)

```
âœ… í•„ìˆ˜:
- AI_eval.py + metadata logging (12 calls)
- Cohen's kappa (2 LLMs, 24 calls total)
- Temperature sensitivity (12 calls)

Total: ~48 API calls, ~$0.15-0.75

ğŸ“ ë…¼ë¬¸ì— í¬í•¨:
- Baseline scores with full metadata
- Cohen's Îº showing inter-model agreement
- Temperature sensitivity showing robustness
```

### ì „ì²´ ë¶„ì„ (ë…¼ë¬¸ ì™„ì„±ë„ ìµœëŒ€í™”)

```
âœ… ì „ì²´:
- AI_eval.py + metadata (12 calls)
- Cohen's kappa (3 LLMs, 36 calls total)
- Temperature sensitivity (48 calls)
- Prompt variations (60 calls)
- Multi-iteration (60 calls)

Total: ~216 API calls, ~$0.65-3.25

ğŸ“ ë…¼ë¬¸ì— í¬í•¨:
- ì™„ì „í•œ robustness analysis
- Multiple mitigation strategies
- Comprehensive validation
```

---

## ğŸ“– ë…¼ë¬¸ ì‘ì„± ê°€ì´ë“œ

### Reviewer Requirement (2) ëŒ€ì‘

**ìš”êµ¬ì‚¬í•­**:
> "Report the exact LLM model/version used, date of calls, API parameters
> (temperature, top_p, max_tokens), how many times each item was evaluated,
> and how results were aggregated (mean/median), and whether sampling was deterministic."

**ëŒ€ì‘ ë°©ë²•**:

1. **llm_metadata_logger.py** ì‚¬ìš©
   - ëª¨ë“  ë©”íƒ€ë°ì´í„°ë¥¼ ìë™ ìˆ˜ì§‘
   - JSON í˜•ì‹ìœ¼ë¡œ ì €ì¥ (ì¬í˜„ ê°€ëŠ¥)
   - ë…¼ë¬¸ Methods ì„¹ì…˜ì— í¬í•¨

**ë…¼ë¬¸ ì˜ˆì‹œ**:
```
We used Claude Sonnet 4.5 (claude-sonnet-4-5-20250929) with deterministic
sampling (temperature=0, max_tokens=1024). Each topic was evaluated once
with batch API calls. All API parameters and timestamps are documented
in our metadata logs (see Supplementary Materials).
```

2. **cohens_kappa_analysis.py** ì‚¬ìš©
   - ì˜ì‚¬ì½”ë“œì™€ ì‹¤ì œ êµ¬í˜„ ëª¨ë‘ ì œê³µ
   - Confusion matrix í¬í•¨
   - ë…¼ë¬¸ Methods ì„¹ì…˜ì— í¬í•¨

**ë…¼ë¬¸ ì˜ˆì‹œ**:
```
To assess inter-rater reliability, we converted continuous LLM scores [0, 1]
to categorical labels using thresholds [0.60, 0.80], yielding three categories:
poor (<0.60), acceptable (0.60-0.80), and excellent (>0.80). We computed
Cohen's Îº = 0.742 (substantial agreement) between Anthropic and OpenAI
evaluators (see Algorithm 1 for pseudocode).
```

### Reviewer Requirement (4) ëŒ€ì‘

**ìš”êµ¬ì‚¬í•­**:
> "Acknowledge LLM bias and hallucination risks, and test robustness: run
> sensitivity analyses across different temperature settings, prompt variations,
> and ideally across â‰¥2 LLMs. Show how much scores fluctuate and discuss
> mitigation strategies."

**ëŒ€ì‘ ë°©ë²•**:

1. **llm_robustness_analysis.py** ì‚¬ìš©
   - Temperature sensitivity (4 values)
   - Prompt variations (5 versions)
   - Multi-LLM comparison (3 models)
   - Statistical analysis + mitigation

**ë…¼ë¬¸ ì˜ˆì‹œ**:
```
We conducted comprehensive robustness testing across temperature settings
(0.0, 0.3, 0.7, 1.0), finding minimal score variation (mean range=0.029,
CV=0.033). Prompt variation analysis (5 semantic versions) showed high
stability (mean range=0.009). Multi-model comparison (Anthropic, OpenAI,
Gemini) revealed strong inter-model correlation (r=0.823), suggesting
low model-specific bias. We recommend temperature=0 for deterministic
evaluation and multi-model ensemble for critical assessments (see
Section 4.5 for mitigation strategies).
```

---

## ğŸ› ï¸ ê°œë°œì ê°€ì´ë“œ

### ìƒˆë¡œìš´ evaluator ì¶”ê°€í•˜ê¸°

1. **base_topic_evaluator.py** ìˆ˜ì •í•˜ì—¬ temperature/prompt ì„¤ì • ê°€ëŠ¥í•˜ê²Œ:

```python
class BaseLLMEvaluator(ABC):
    def __init__(self):
        self.system_prompt = """..."""
        self._temperature = 0  # Default

    def set_temperature(self, temperature: float):
        """Set temperature for API calls"""
        self._temperature = temperature

    def set_system_prompt(self, prompt: str):
        """Set custom system prompt"""
        self.system_prompt = prompt
```

2. **ê° evaluator** (anthropic, openai, etc.)ì—ì„œ temperature ì‚¬ìš©:

```python
def _call_api(self, metric: str, prompt: str) -> str:
    message = self.client.messages.create(
        model=self.model,
        temperature=self._temperature,  # Use instance variable
        # ...
    )
```

### ì»¤ìŠ¤í…€ ë¶„ì„ ì¶”ê°€í•˜ê¸°

```python
# custom_analysis.py
from topic_llm.llm_robustness_analysis import RobustnessAnalyzer

class CustomAnalyzer(RobustnessAnalyzer):
    def custom_analysis(self, ...):
        # Your custom analysis logic
        pass
```

---

## ğŸ“š ì°¸ê³ ë¬¸í—Œ

- **Cohen's Kappa**: Landis, J. R., & Koch, G. G. (1977). The measurement of observer agreement for categorical data. *Biometrics*, 33(1), 159-174.
- **Inter-rater Reliability**: Fleiss, J. L., Levin, B., & Paik, M. C. (2003). *Statistical methods for rates and proportions*. John Wiley & Sons.
- **LLM Evaluation**: Zheng, L., et al. (2023). Judging LLM-as-a-judge with MT-bench and chatbot arena. *NeurIPS*.

---

## âœ… ì²´í¬ë¦¬ìŠ¤íŠ¸

ë…¼ë¬¸ ì œì¶œ ì „ í™•ì¸ì‚¬í•­:

- [ ] AI_eval.py ì‹¤í–‰ ë° baseline ê²°ê³¼ í™•ë³´
- [ ] llm_metadata_logger í†µí•© ë° ë©”íƒ€ë°ì´í„° ì €ì¥
- [ ] cohens_kappa_analysisë¡œ inter-rater reliability ê³„ì‚°
- [ ] Temperature sensitivity ë¶„ì„ ì™„ë£Œ
- [ ] Multi-LLM comparison ì™„ë£Œ (2ê°œ ì´ìƒ)
- [ ] Mitigation strategies ë¬¸ì„œí™”
- [ ] ëª¨ë“  ê²°ê³¼ë¥¼ ë…¼ë¬¸ Methods/Results ì„¹ì…˜ì— í†µí•©
- [ ] Supplementary materialsì— ì „ì²´ ë¡œê·¸ í¬í•¨
- [ ] ì¬í˜„ ê°€ëŠ¥ì„± í™•ë³´ (ì½”ë“œ + ë°ì´í„° + ë©”íƒ€ë°ì´í„°)

---

## ğŸ†˜ ë¬¸ì œ í•´ê²°

### Q1: Temperature ì„¤ì •ì´ ì‘ë™í•˜ì§€ ì•ŠìŒ
**A**: `base_topic_evaluator.py`ì™€ ê° evaluatorì— `set_temperature()` ë©”ì„œë“œë¥¼ ì¶”ê°€í•´ì•¼ í•©ë‹ˆë‹¤. (ìœ„ì˜ ê°œë°œì ê°€ì´ë“œ ì°¸ì¡°)

### Q2: API ë¹„ìš©ì´ ë„ˆë¬´ ë†’ìŒ
**A**:
- ë‹¨ì¼ dataset (distinct)ë§Œ í…ŒìŠ¤íŠ¸
- Temperature sweepë§Œ ì‹¤í–‰ (ê°€ì¥ ì¤‘ìš”)
- Prompt variation ìƒëµ (ì„ íƒì‚¬í•­)
- Multi-iterationì„ 3íšŒë¡œ ì¤„ì„

### Q3: Gemini evaluatorê°€ ì—†ìŒ
**A**:
- Anthropic + OpenAIë§Œ ì‚¬ìš© (ì¶©ë¶„)
- ë˜ëŠ” `gemini_topic_evaluator.py` ìƒì„± í•„ìš”

### Q4: ê²°ê³¼ ì¬í˜„ì´ ì•ˆë¨
**A**:
- Temperature=0 ì‚¬ìš© í™•ì¸
- ë©”íƒ€ë°ì´í„° JSON íŒŒì¼ í™•ì¸
- ë™ì¼í•œ ëª¨ë¸ ë²„ì „ ì‚¬ìš© í™•ì¸

---

## ğŸ“§ ì—°ë½ì²˜

ì§ˆë¬¸ì´ë‚˜ ë¬¸ì œê°€ ìˆìœ¼ë©´ ì´ìŠˆë¥¼ ì œì¶œí•˜ê±°ë‚˜ ê°œë°œìì—ê²Œ ì—°ë½í•˜ì„¸ìš”.

**Generated**: 2025-01-17
**Version**: 1.0.0
