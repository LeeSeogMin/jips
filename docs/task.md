
â— ğŸ“‹ Comments.md ë³´ì™„ ì‘ì—… ì¢…í•© ê³„íš

  ê¸°ì¤€ ë°ì´í„°: 2024ë…„ 10ì›” 8ì¼ ì¶”ì¶œ Wikipedia ë°ì´í„°ëª©í‘œ: Comments.mdì˜ ëª¨ë“         
  Major/Minor Issues ì™„ë²½ í•´ê²°

  ---
  ğŸ¯ Phase 1: ë°ì´í„° ë©”íƒ€ì •ë³´ ë¬¸ì„œí™” (1-2ì‹œê°„)

  ì‘ì—… ë‚´ìš©

  1. í¬ë¡¤ ë‚ ì§œ ëª…ì‹œ: 2024ë…„ 10ì›” 8ì¼ Wikipedia API ì¶”ì¶œ
  2. ì¿¼ë¦¬ ì‹œë“œ ë¬¸ì„œí™”: ê° ë°ì´í„°ì…‹ë³„ í† í”½ í‚¤ì›Œë“œ ì¶”ì¶œ
  3. í•„í„°ë§ ê·œì¹™ ì •ë¦¬: ë¬¸ì„œ ì„ íƒ ê¸°ì¤€, ì „ì²˜ë¦¬ ê³¼ì •
  4. ì˜ˆì‹œ ë¬¸ì„œ ì œê³µ: ê° í† í”½ë‹¹ 1-2ê°œ ëŒ€í‘œ ë¬¸ì„œ ìƒ˜í”Œ

  ì‚°ì¶œë¬¼

  - dataset_metadata.md: ë°ì´í„°ì…‹ êµ¬ì¶• ì™„ì „ ì¬í˜„ ê°€ì´ë“œ
  - sample_documents.txt: í† í”½ë³„ ì˜ˆì‹œ ë¬¸ì„œ

  ì‚¬ìš© ë„êµ¬

  # ë°ì´í„° íƒìƒ‰
  pandas (distinct_topic.csv, similar_topic.csv, more_similar_topic.csv)
  # ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
  pickle files (embeddings, topics)

  ---
  ğŸ“Š Phase 2: ëª¨ë“  í‰ê°€ ì§€í‘œ ì¬ê³„ì‚° (3-4ì‹œê°„)

  2.1 Statistical Metrics ì¬ê³„ì‚°

  ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸: StatEvaluator.py, ST_Eval.py

  ì¬ê³„ì‚° ì§€í‘œ:
  - NPMI Coherence
  - C_v Coherence
  - KLD Distinctiveness
  - TD & IRBO Diversity

  ì…ë ¥ ë°ì´í„°:
  data/topics_distinct_tfidf.pkl
  data/topics_similar_tfidf.pkl
  data/topics_more_similar_tfidf.pkl

  2.2 Semantic Metrics ì¬ê³„ì‚°

  ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸: NeuralEvaluator.py, DL_Eval.py

  ì¬ê³„ì‚° ì§€í‘œ:
  - Semantic Coherence (SC)
  - Semantic Distinctiveness (SD)
  - Semantic Diversity (SemDiv)

  ì…ë ¥ ë°ì´í„°:
  data/embeddings_distinct.pkl (384-dim)
  data/topics_distinct.pkl
  data/bert_outputs_distinct.pkl

  2.3 LLM ê¸°ë°˜ í‰ê°€ ì¬ì‹¤í–‰

  **2ê°œì˜ ë…ë¦½ì ì¸ LLM í‰ê°€ ì‹œìŠ¤í…œ**:

  **A. í† í”½ ëª¨ë¸ë§ ì§€í‘œ í‰ê°€ (Topic Metrics Evaluation)**

  ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸:
  - llm_analyzers/openai_topic_evaluator.py (GPT-4.1)
  - llm_analyzers/anthropic_topic_evaluator.py (Claude Sonnet 4.5)
  - llm_analyzers/gemini_topic_evaluator.py (Gemini 2.5 Flash)
  - llm_analyzers/grok_topic_evaluator.py (Grok 4)

  ì‚¬ìš© ëª¨ë¸ (4-LLM Framework):
  1. **OpenAI GPT-4.1** (`gpt-4.1`)
  2. **Anthropic Claude Sonnet 4.5** (`claude-sonnet-4-5-20250929`)
  3. **Google Gemini 2.5 Flash** (`gemini-2.5-flash-preview-09-2025`)
  4. **xAI Grok 4** (`grok-4-0709`)

  í‰ê°€ ë‚´ìš©:
  - Coherence, Distinctiveness, Diversity, Semantic Integration (4ê°€ì§€ ì°¨ì›)
  - í† í”½ í‚¤ì›Œë“œ ì„¸íŠ¸ì— ëŒ€í•œ í’ˆì§ˆ í‰ê°€
  - 4ê°œ LLMì˜ í‰ê°€ ì ìˆ˜ ë° ì„¤ëª… ìƒì„±
  - Multi-model Cohen's Îº ê³„ì‚° (4Ã—4 agreement matrix)
  - Fleiss' Îº ì¶”ê°€ (4ê°œ í‰ê°€ììš©)

  ì…ë ¥ ë°ì´í„°:
  - data/topics_distinct.pkl
  - data/topics_similar.pkl
  - data/topics_more_similar.pkl

  ìƒˆë¡œìš´ ìš”êµ¬ì‚¬í•­:
  - **Temperature í…ŒìŠ¤íŠ¸**: 0.0, 0.3, 0.7, 1.0 (ê° ëª¨ë¸ë³„)
  - **Prompt variants**: 3ê°œ ë²„ì „ (ëª¨ë“  ëª¨ë¸ ë™ì¼ í”„ë¡¬í”„íŠ¸)
  - **Multi-run**: ê° ì¡°ê±´ë‹¹ 3íšŒ ì‹¤í–‰ (ì¬í˜„ì„± ê²€ì¦)

  ì‚°ì¶œë¬¼

  - data/openai_evaluation_results.pkl: OpenAI í‰ê°€ ê²°ê³¼
  - data/anthropic_evaluation_results.pkl: Claude í‰ê°€ ê²°ê³¼
  - data/gemini_evaluation_results.pkl: Gemini í‰ê°€ ê²°ê³¼
  - data/grok_evaluation_results.pkl: Grok í‰ê°€ ê²°ê³¼
  - recalculated_metrics.csv: ëª¨ë“  ì§€í‘œ í†µí•© ê²°ê³¼
  - llm_agreement_metrics.json: Cohen's Îº (4Ã—4) & Fleiss' Îº

  ---
  ğŸ”¢ Phase 3: ìˆ«ì í†µì¼ (1ì‹œê°„)

  ì‘ì—… ë‚´ìš©

  Phase 2ì˜ ì¬ê³„ì‚° ê²°ê³¼ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ëª¨ë“  í†µê³„ì¹˜ í†µì¼

  í†µì¼ ëŒ€ìƒ:
  1. Cohen's Îº â†’ ì¬ê³„ì‚° ê²°ê³¼ë¡œ ë‹¨ì¼ ê°’ ê²°ì •
  2. r (semantic-LLM) â†’ ì¬ê³„ì‚° ê²°ê³¼ë¡œ í†µì¼
  3. r (traditional-LLM) â†’ ì¬ê³„ì‚° ê²°ê³¼ë¡œ í†µì¼
  4. 27.3% accuracy â†’ ê²€ì¦
  5. 36.5% discriminative power â†’ ê²€ì¦

  ê²€ì¦ ì ˆì°¨:

  # ì˜ˆì‹œ ê²€ì¦ ì½”ë“œ
  def verify_correlations():
      semantic_scores = load_semantic_results()
      llm_scores = load_llm_results()
      r_semantic_llm = pearsonr(semantic_scores, llm_scores)[0]

      traditional_scores = load_statistical_results()
      r_traditional_llm = pearsonr(traditional_scores, llm_scores)[0]

      return {
          'r_semantic_llm': round(r_semantic_llm, 2),
          'r_traditional_llm': round(r_traditional_llm, 2)
      }

  ì‚°ì¶œë¬¼

  - unified_statistics.json: ëª¨ë“  í†µê³„ì¹˜ ë‹¨ì¼ ì°¸ì¡° íŒŒì¼
  - number_verification_report.md: í†µì¼ ê·¼ê±° ë° ê²€ì¦ ê³¼ì •

  ---
  ğŸ”§ Phase 4: ë©”íŠ¸ë¦­ íŒŒë¼ë¯¸í„° ëª…ì‹œ (2ì‹œê°„)

  ì‘ì—… ë‚´ìš©

  4.1 ì‚¬ìš©ëœ ì‹¤ì œ ê°’ ì¶”ì¶œ

  # ì½”ë“œì—ì„œ ì‹¤ì œ ì‚¬ìš© ê°’ í™•ì¸
  grep -r "lambda\|alpha\|beta\|gamma" *.py

  # NeuralEvaluator.py, DL_Eval.py ë¶„ì„

  4.2 íŒŒë¼ë¯¸í„° ê²°ì • ê·¼ê±° ë¬¸ì„œí™”

  | íŒŒë¼ë¯¸í„° | ì‹¤ì œ ê°’           | ì„ íƒ ê·¼ê±°                    | ë²”ìœ„
        |
  |------|----------------|--------------------------|-------------------|
  | Î»w   | TF-IDF weights | ë‹¨ì–´ ì¤‘ìš”ë„ ë°˜ì˜                | [0, 1] normalized     
  |
  | Î³    | 0.5 (ì˜ˆìƒ)       | Balancing factor         | [0, 1]            |        
  | Î±    | 0.6 (ì˜ˆìƒ)       | Vector diversity weight  | [0, 1]            |        
  | Î²    | 0.4 (ì˜ˆìƒ)       | Content diversity weight | [0, 1], Î±+Î²=1     |        

  4.3 Toy Example ì‘ì„±

  ### Semantic Coherence Calculation Example

  **Topic T**: ["machine", "learning", "algorithm"]

  **Step 1**: Get word embeddings
  - e_machine = [0.12, -0.34, ..., 0.56] (384-dim)
  - e_learning = [0.15, -0.29, ..., 0.61]
  - e_algorithm = [0.18, -0.31, ..., 0.58]

  **Step 2**: Compute topic embedding
  - e_T = mean([e_machine, e_learning, e_algorithm])

  **Step 3**: Calculate similarities
  - sim(e_machine, e_T) = 0.92
  - sim(e_learning, e_T) = 0.88
  - sim(e_algorithm, e_T) = 0.90

  **Step 4**: Apply weights (Î»w from TF-IDF)
  - Î»_machine = 0.35, Î»_learning = 0.40, Î»_algorithm = 0.25

  **Step 5**: Compute SC
  SC(T) = (0.35Ã—0.92 + 0.40Ã—0.88 + 0.25Ã—0.90) / 3
        = (0.322 + 0.352 + 0.225) / 3
        = 0.899 / 3
        = **0.300**

  ì‚°ì¶œë¬¼

  - metric_parameters.md: ëª¨ë“  íŒŒë¼ë¯¸í„° ì •ì˜ ë° ê°’
  - toy_examples.md: 3ê°œ ë©”íŠ¸ë¦­ ê³„ì‚° ì˜ˆì‹œ (SC, SD, SemDiv)

  ---
  ğŸ§ª Phase 5: LLM Robustness í…ŒìŠ¤íŠ¸ (4-5ì‹œê°„)

  5.1 Temperature Sensitivity Test

  # llm_robustness_test.py
  temperatures = [0.0, 0.3, 0.7, 1.0]
  for temp in temperatures:
      scores = evaluate_topics(gpt4, topics, temperature=temp)
      save_results(f"temp_{temp}_results.json")

  í‰ê°€:
  - ê° temperatureë³„ coherence, distinctiveness, diversity ì ìˆ˜
  - ì ìˆ˜ ë³€ë™ ë¶„ì„ (std, CV)

  5.2 Prompt Variant Test

  3ê°€ì§€ Prompt:
  1. Original: "You are an expert in topic modeling..."
  2. Variant A: "As a domain specialist in computational linguistics..."
  3. Variant B: "Evaluate the following topic model objectively..."

  5.3 Multi-model Comparison

  - GPT
  - Claude
  - grok 

  ì‚°ì¶œë¬¼

  - robustness_test_results.csv: ëª¨ë“  ì¡°í•© ê²°ê³¼
  - llm_robustness_analysis.md:
    - Temperature ì˜í–¥ ë¶„ì„
    - Prompt sensitivity ë¶„ì„
    - Model agreement ë¶„ì„
    - Disagreement case ë¶„ì„

  5.4 LLM Limitations ë…¼ì˜

  ### LLM Bias and Hallucination Risks

  **LLM Bias Analysis**:
  1. **Domain Bias**:
     - LLMì´ íŠ¹ì • í•™ë¬¸ ë¶„ì•¼ì— í¸í–¥ë  ìˆ˜ ìˆìŒ
     - ì˜ˆ: ì»´í“¨í„°ê³¼í•™ í† í”½ì— ë” ë†’ì€ ì ìˆ˜ ë¶€ì—¬ ê°€ëŠ¥ì„±
  2. **Length Bias**:
     - í‚¤ì›Œë“œ ìˆ˜ê°€ ë§ì€ í† í”½ì— ìœ ë¦¬í•  ìˆ˜ ìˆìŒ
     - ë¶„ì„: í‚¤ì›Œë“œ ìˆ˜ vs LLM ì ìˆ˜ ìƒê´€ê´€ê³„ ê²€ì¦

  **Hallucination Risk**:
  1. **ì „ë¬¸ ë„ë©”ì¸ ìœ„í—˜ì„±**:
     - LLMì´ ìƒì†Œí•œ ì „ë¬¸ ìš©ì–´ ì¡°í•©ì„ ì˜ëª» í•´ì„
     - ì˜ˆ: ì˜í•™, ë²•ë¥  ë“± ì „ë¬¸ ë¶„ì•¼ í† í”½
  2. **í‰ê°€ ê·¼ê±° ê²€ì¦**:
     - LLM ì„¤ëª…(explanation)ì˜ íƒ€ë‹¹ì„± ê²€ì¦ í•„ìš”

  **Mitigation Strategies**:
  1. **Multi-model Consensus**:
     - 4ê°œ LLM í‰ê°€ ê²°ê³¼ êµì°¨ ê²€ì¦
     - Fleiss' Îºë¡œ í‰ê°€ì ê°„ ì¼ì¹˜ë„ ì¸¡ì •
  2. **Statistical Validation**:
     - LLM í‰ê°€ì™€ ì „í†µì  ì§€í‘œ ê°„ ìƒê´€ê´€ê³„ ê²€ì¦
     - r(semantic-LLM), r(traditional-LLM) ë¶„ì„
  3. **Human Validation** (ì„ íƒì ):
     - ë¶ˆì¼ì¹˜ ì¼€ì´ìŠ¤ì— ëŒ€í•œ ì „ë¬¸ê°€ ê²€ì¦

  **Section 6 Limitations ì¶”ê°€ ë‚´ìš©**:
  - LLM ê³ ìœ ì˜ í¸í–¥ì„±(bias) ë° í™˜ê°(hallucination) ê°€ëŠ¥ì„±
  - ì „ë¬¸ ë„ë©”ì¸ì—ì„œì˜ í‰ê°€ ì •í™•ë„ í•œê³„
  - Multi-model consensusë¥¼ í†µí•œ ì™„í™” ì „ëµ ì ìš©

  ì‚°ì¶œë¬¼

  - llm_bias_analysis.md: í¸í–¥ì„± ë¶„ì„ ê²°ê³¼
  - hallucination_cases.md: í™˜ê° ì‚¬ë¡€ ë¶„ì„
  - mitigation_effectiveness.csv: ì™„í™” ì „ëµ íš¨ê³¼ ê²€ì¦

  ---
  ğŸ“– Phase 6: ì¬í˜„ì„± ë³´ê³ ì„œ ì‘ì„± (2ì‹œê°„)

  6.1 Embedding Model ëª…ì‹œ

  ### Embedding Model Specification

  **Model**: `sentence-transformers/all-MiniLM-L6-v2`
  **Version**: v2.2.0
  **Dimensions**: 384
  **Tokenizer**: WordPiece (bert-base-uncased)
  **Max Sequence Length**: 256 tokens

  **Pre-processing**:
  1. Lowercasing: Yes
  2. Stopword removal: No
  3. Lemmatization: No
  4. Special token handling: [CLS], [SEP] added

  **Hugging Face**: `sentence-transformers/all-MiniLM-L6-v2`
  **Download Command**:
  ```python
  from sentence_transformers import SentenceTransformer
  model = SentenceTransformer('all-MiniLM-L6-v2')

  6.2 LLM API Parameters

  ### LLM Evaluation Parameters

  **GPT**:
  - Model: ``
  - Temperature: 0.7 (default), [0.0, 0.3, 1.0 for robustness]
  - top_p: 1.0
  - max_tokens: 500
  - Evaluation date: 2024-10-XX to 2024-10-XX

  **Claud**:
  - Model: ``
  - Temperature: 0.7
  - top_p: 1.0
  - max_tokens: 500
  - Evaluation date: Same as GPT-4

  **Aggregation**:
  - Each topic evaluated 3 times
  - Aggregation: median score
  - Categorical conversion: bins=[0, 0.33, 0.67, 1.0]

  6.3 Dataset Construction

  ### Dataset Construction Methodology

  **Data Source**: Wikipedia API
  **Extraction Date**: 2024-10-08
  **Query Strategy**: Topic-based seed page crawling

  **Distinct Dataset**:
  - Seed topics: ["Evolution", "Classical_mechanics", "Molecular_biology", ...]
  - Filter: Scientific domain, min 20 words, max 500 words
  - Total: 3,445 documents, 15 topics

  **Processing Pipeline**:
  1. Wikipedia API query with seed pages
  2. Text extraction (intro + summary sections)
  3. Cleaning: Remove citations, references, links
  4. Length filtering: 20-500 words
  5. Duplicate removal: Cosine similarity < 0.9

  6.4 Visualization Parameters

  ### t-SNE Hyperparameters

  **t-SNE Configuration**:
  - perplexity: 30
  - learning_rate: 200
  - n_iter: 1000
  - random_state: 42
  - metric: cosine

  **UMAP Comparison** (Alternative visualization):
  - n_neighbors: 15
  - min_dist: 0.1
  - metric: cosine
  - random_state: 42

  **Stability Verification**:
  - Multiple seeds: [42, 123, 456]
  - Visual consistency check across seeds
  - Report any significant layout variations

  ì‚°ì¶œë¬¼

  - reproducibility_guide.md: ì™„ì „ ì¬í˜„ ê°€ì´ë“œ
  - requirements.txt: ëª¨ë“  ì˜ì¡´ì„± ë²„ì „
  - reproduction_script.py: 1-click ì¬í˜„ ìŠ¤í¬ë¦½íŠ¸
  - visualization_parameters.json: t-SNE & UMAP ì„¤ì •

  ---
  ğŸ“ Phase 7: Toy Example ìƒì„± (1-2ì‹œê°„)

  Phase 4ì—ì„œ ì‘ì„±í•œ Toy Exampleì„ í™•ì¥:

  Appendix B ì¶”ê°€ ë‚´ìš©

  - B.1: Semantic Coherence ê³„ì‚° ì˜ˆì‹œ
  - B.2: Semantic Distinctiveness ê³„ì‚° ì˜ˆì‹œ
  - B.3: Semantic Diversity ê³„ì‚° ì˜ˆì‹œ
  - B.4: Statistical vs Semantic ë¹„êµ ì˜ˆì‹œ

  ê° ì˜ˆì‹œ:
  - ì‹¤ì œ ë°ì´í„°ì—ì„œ ì¶”ì¶œí•œ í† í”½ ì‚¬ìš©
  - Step-by-step ê³„ì‚° ê³¼ì •
  - ì¤‘ê°„ ê²°ê³¼ê°’ í‘œì‹œ
  - ìµœì¢… ê²°ê³¼ í•´ì„

  ---
  ğŸ“„ Phase 8: Manuscript ì—…ë°ì´íŠ¸ (3-4ì‹œê°„)

  8.1 Section 3.1 ì—…ë°ì´íŠ¸ (Dataset Construction)

  ì¶”ê°€ ë‚´ìš©:
  - Extraction date: October 8, 2024
  - Query strategy: í† í”½ë³„ ì‹œë“œ í˜ì´ì§€ + BFS crawling
  - Filtering rules: ê¸¸ì´, í’ˆì§ˆ, ì¤‘ë³µ ì œê±° ê¸°ì¤€
  - Example documents: Appendixì— í† í”½ë³„ ìƒ˜í”Œ 2ê°œ

  8.2 Section 3.2 ì—…ë°ì´íŠ¸ (Embedding Model)

  ì¶”ê°€ ë‚´ìš©:
  - Model: sentence-transformers/all-MiniLM-L6-v2
  - Tokenizer, pre-processing details
  - Hyperparameters: max_length=256

  8.3 Section 3.3 ì—…ë°ì´íŠ¸ (Metric Parameters)

  ì¶”ê°€ ë‚´ìš©:
  - Î»w = TF-IDF weights (range: [0,1], normalized)
  - Î± = 0.6, Î² = 0.4 (sum=1, empirically determined via grid search)
  - Î³ = 0.5 (balancing hierarchical overlap)
  - Toy examples (Appendix B ì°¸ì¡°)

  8.4 Section 4.4 ì—…ë°ì´íŠ¸ (LLM Evaluation)

  ì¶”ê°€ ë‚´ìš©:
  - LLM API parameters (temperature=0.7, top_p=1.0)
  - Evaluation date range
  - Aggregation method (median of 3 runs)
  - Robustness tests (Appendix C ì°¸ì¡°)

  8.5 Section 6 ì—…ë°ì´íŠ¸ (Conclusion & Limitations)

  í™•ì¥ ë‚´ìš©:
  Limitations:
  1. Computational complexity (2.3x slower than statistical)
  2. English-only corpus validation
  3. LLM dependency (cost, availability, potential bias)
  4. Temperature sensitivity (Îº variance: 0.02-0.05)
  5. Synthetic Wikipedia data (need real-world validation)

  8.6 ìˆ«ì í†µì¼ (All Sections)

  - Abstract, Results, Discussion, Conclusion ëª¨ë“  í†µê³„ì¹˜ í†µì¼
  - Phase 3ì˜ unified_statistics.json ê¸°ì¤€ ì ìš©

  8.7 Appendices ì¶”ê°€

  - Appendix B: Toy Examples (metric calculations)
  - Appendix C: LLM Robustness Tests
  - Appendix D: Dataset Construction Details
  - Appendix E: Reproducibility Checklist

  8.9 Related Work ê°•í™”

  ### Section 2.2 ì—…ë°ì´íŠ¸: Related Work ì°¨ë³„ì„± ëª…í™•í™”

  **Ref. 15 (LLM-based Evaluation) vs ë³¸ ì—°êµ¬ ë¹„êµ**:

  | ì¸¡ë©´ | Ref. 15 | ë³¸ ì—°êµ¬ |
  |------|---------|---------|
  | í‰ê°€ ë°©ì‹ | LLM ë‹¨ë… í‰ê°€ | Statistical + Semantic + LLM í†µí•© |
  | ê²€ì¦ ë°©ë²• | ë‹¨ì¼ ëª¨ë¸ | 4-LLM consensus (Fleiss' Îº) |
  | ë©”íŠ¸ë¦­ ë²”ìœ„ | LLM ì£¼ê´€ í‰ê°€ | 12ê°œ ì§€í‘œ (í†µê³„ 6 + ì˜ë¯¸ 3 + LLM 3) |
  | ì¬í˜„ì„± | ì œí•œì  | ì™„ì „ ì¬í˜„ ê°€ëŠ¥ (ìƒì„¸ íŒŒë¼ë¯¸í„° ëª…ì‹œ) |
  | Robustness | ë¯¸ê²€ì¦ | Temperature/Prompt sensitivity í…ŒìŠ¤íŠ¸ |

  **ë³¸ ì—°êµ¬ì˜ ì°¨ë³„ì  ë° ì¤‘ìš”ì„±**:
  1. **Comprehensive Validation**:
     - ë‹¨ì¼ ë°©ë²•ë¡ ì´ ì•„ë‹Œ 3ê°€ì§€ ì ‘ê·¼ë²• êµì°¨ ê²€ì¦
     - Statistical (NPMI, C_v) + Semantic (SC, SD) + LLM (4-model consensus)
  2. **Multi-model Consensus**:
     - Ref. 15ëŠ” ë‹¨ì¼ LLM ì˜ì¡´
     - ë³¸ ì—°êµ¬ëŠ” 4ê°œ LLM í‰ê°€ì ê°„ ì¼ì¹˜ë„(Fleiss' Îº) ì¸¡ì •
  3. **Reproducibility**:
     - Embedding model, LLM parameters, dataset construction ì™„ì „ ëª…ì‹œ
     - Ref. 15ëŠ” ì¬í˜„ì„± ì •ë³´ ì œí•œì 
  4. **Robustness Analysis**:
     - Temperature sensitivity, prompt variants ì²´ê³„ì  í…ŒìŠ¤íŠ¸
     - Ref. 15ëŠ” robustness ë¯¸ê²€ì¦

  **Why More Important**:
  - ì‹¤ë¬´ ì ìš© ì‹œ ì‹ ë¢°ì„±: ë‹¤ì¸µ ê²€ì¦ìœ¼ë¡œ í‰ê°€ ì‹ ë¢°ë„ í–¥ìƒ
  - ì¬í˜„ ê°€ëŠ¥ì„±: ì™„ì „í•œ ì¬í˜„ ê°€ì´ë“œë¡œ í›„ì† ì—°êµ¬ ì´‰ì§„
  - ë¹„ìš©-íš¨ê³¼: LLM ë‹¨ë…ë³´ë‹¤ statistical/semantic ë³‘í–‰ìœ¼ë¡œ ë¹„ìš© ì ˆê°

  ì‚°ì¶œë¬¼

  - related_work_comparison.md: Ref. 15ì™€ì˜ ìƒì„¸ ë¹„êµí‘œ
  - differentiation_rationale.md: ì°¨ë³„ì„± ë° ì¤‘ìš”ì„± ë…¼ê±°

  8.10 Terminology Consistency

  ### ì•½ì–´ ë° ì „ë¬¸ ìš©ì–´ ê²€ì¦

  **ì•½ì–´ ì •ì˜ ì²´í¬ë¦¬ìŠ¤íŠ¸**:

  | ì•½ì–´ | ì „ì²´ ëª…ì¹­ | ì²« ì‚¬ìš© ìœ„ì¹˜ | ì •ì˜ ì—¬ë¶€ |
  |------|-----------|-------------|-----------|
  | NPMI | Normalized Pointwise Mutual Information | Section 3.1 | âœ“ í™•ì¸ í•„ìš” |
  | IRBO | Inverted Rank-Biased Overlap | Section 3.1 | âœ“ í™•ì¸ í•„ìš” |
  | RBO | Rank-Biased Overlap | Section 3.1 | âœ“ í™•ì¸ í•„ìš” |
  | SC | Semantic Coherence | Section 3.2 | âœ“ í™•ì¸ í•„ìš” |
  | SD | Semantic Distinctiveness | Section 3.2 | âœ“ í™•ì¸ í•„ìš” |
  | SemDiv | Semantic Diversity | Section 3.2 | âœ“ í™•ì¸ í•„ìš” |
  | LLM | Large Language Model | Abstract | âœ“ í™•ì¸ í•„ìš” |
  | BERTopic | BERT-based Topic Model | Section 2.1 | âœ“ í™•ì¸ í•„ìš” |

  **ê²€ì¦ ì ˆì°¨**:
  1. Manuscript ì „ì²´ í…ìŠ¤íŠ¸ ê²€ìƒ‰
  2. ê° ì•½ì–´ì˜ ì²« ë“±ì¥ ìœ„ì¹˜ í™•ì¸
  3. ì²« ì‚¬ìš© ì‹œ full name ì •ì˜ ì—¬ë¶€ ì²´í¬
  4. ì •ì˜ ëˆ„ë½ ì‹œ ì¶”ê°€

  **ì˜ˆì‹œ ìˆ˜ì •**:
  ```
  AS-IS:
  "We use IRBO to measure topic diversity..."

  TO-BE:
  "We use Inverted Rank-Biased Overlap (IRBO) to measure topic diversity..."
  ```

  ì‚°ì¶œë¬¼

  - terminology_checklist.csv: ëª¨ë“  ì•½ì–´ ê²€ì¦ ê²°ê³¼
  - abbreviation_definitions.md: ì•½ì–´ ì „ì²´ ëª©ë¡ ë° ì •ì˜

  ---
  ğŸ“¦ ìµœì¢… ì‚°ì¶œë¬¼ ì²´í¬ë¦¬ìŠ¤íŠ¸

  Code & Data

  - recalculated_metrics.csv: ëª¨ë“  ì§€í‘œ ê²°ê³¼
  - unified_statistics.json: í†µí•© í†µê³„ì¹˜
  - metric_parameters.json: íŒŒë¼ë¯¸í„° ê°’
  - robustness_test_results.csv: Robustness ê²°ê³¼
  - reproduction_script.py: ì¬í˜„ ìŠ¤í¬ë¦½íŠ¸

  Documentation

  - dataset_metadata.md: ë°ì´í„°ì…‹ ë©”íƒ€ì •ë³´
  - reproducibility_guide.md: ì¬í˜„ ê°€ì´ë“œ
  - metric_parameters.md: íŒŒë¼ë¯¸í„° ì •ì˜
  - toy_examples.md: ê³„ì‚° ì˜ˆì‹œ
  - llm_robustness_analysis.md: Robustness ë¶„ì„
  - number_verification_report.md: ìˆ«ì ê²€ì¦
  - visualization_parameters.json: t-SNE & UMAP ì„¤ì •
  - llm_bias_analysis.md: LLM í¸í–¥ì„± ë¶„ì„
  - hallucination_cases.md: í™˜ê° ì‚¬ë¡€ ë¶„ì„
  - related_work_comparison.md: Ref. 15 ë¹„êµ
  - terminology_checklist.csv: ì•½ì–´ ê²€ì¦ ê²°ê³¼

  Manuscript Updates

  - Section 2.2: Related Work ì°¨ë³„ì„± ëª…í™•í™” (Phase 8.9)
  - Section 3.1: Dataset construction details
  - Section 3.2: Embedding model specification
  - Section 3.3: Metric parameters
  - Section 4.4: LLM evaluation details
  - Section 5: Robustness discussion
  - Section 6: Extended limitations (LLM bias/hallucination ì¶”ê°€)
  - All numerical values unified
  - All abbreviations defined at first use (Phase 8.10)
  - Appendices B, C, D, E added

  ---
  â±ï¸ ì˜ˆìƒ ì†Œìš” ì‹œê°„

  | Phase   | ì‘ì—… ë‚´ìš©              | ì‹œê°„   | ëˆ„ì   |
  |---------|--------------------|------|-----|
  | Phase 1 | ë°ì´í„° ë©”íƒ€ì •ë³´ ë¬¸ì„œí™”       | 1-2h | 2h  |
  | Phase 2 | ëª¨ë“  í‰ê°€ ì§€í‘œ ì¬ê³„ì‚°       | 3-4h | 6h  |
  | Phase 3 | ìˆ«ì í†µì¼              | 1h   | 7h  |
  | Phase 4 | ë©”íŠ¸ë¦­ íŒŒë¼ë¯¸í„° ëª…ì‹œ        | 2h   | 9h  |
  | Phase 5 | LLM Robustness í…ŒìŠ¤íŠ¸ | 4-5h | 14h |
  | Phase 6 | ì¬í˜„ì„± ë³´ê³ ì„œ ì‘ì„±         | 2h   | 16h |
  | Phase 7 | Toy Example ìƒì„±     | 1-2h | 18h |
  | Phase 8 | Manuscript ì—…ë°ì´íŠ¸    | 3-4h | 22h |

  ì´ ì˜ˆìƒ ì‹œê°„: 18-22ì‹œê°„ (2-3ì¼ ì‘ì—…)

  ---
  ğŸš€ ì‹¤í–‰ ìš°ì„ ìˆœìœ„

  High Priority (Major Issues í•´ê²°)

  1. âœ… Phase 2: ì§€í‘œ ì¬ê³„ì‚° (ìˆ«ì ë¶ˆì¼ì¹˜ í•´ê²° ê¸°ë°˜)
  2. âœ… Phase 3: ìˆ«ì í†µì¼ (Critical Issue 1 í•´ê²°)
  3. âœ… Phase 4: íŒŒë¼ë¯¸í„° ëª…ì‹œ (Critical Issue 3 í•´ê²°)
  4. âœ… Phase 6: ì¬í˜„ì„± ë¬¸ì„œí™” (Critical Issue 2 í•´ê²°)

  Medium Priority (Robustness & Quality)

  5. âœ… Phase 5: LLM Robustness (Critical Issue 4 í•´ê²°)
  6. âœ… Phase 7: Toy Examples (ë¦¬ë·°ì–´ ìš”ì²­ í•´ê²°)

  Completion

  7. âœ… Phase 1: ë©”íƒ€ì •ë³´ ë¬¸ì„œí™” (ì§€ì› ìë£Œ)
  8. âœ… Phase 8: Manuscript í†µí•© ì—…ë°ì´íŠ¸




    ë…¼ë¬¸ manuscript ì—…ë°ì´íŠ¸ê°€ í•„ìš”í•œ ê²½ìš°, phase6_7_8_completion_report.mdì˜      
  Phase 8 Checklistë¥¼ ì°¸ì¡°í•˜ì—¬:

  1. Section 3.1 (Dataset Construction) ì—…ë°ì´íŠ¸
  2. Section 3.2 (Embedding Model) ì—…ë°ì´íŠ¸
  3. Section 3.3 (Metric Parameters) ì—…ë°ì´íŠ¸
  4. Section 4.4 (LLM Evaluation) ì—…ë°ì´íŠ¸
  5. Section 5 (Robustness) ì¶”ê°€
  6. Section 6 (Limitations) í™•ì¥
  7. Appendices B, C, D, E ì¶”ê°€
  8. ìˆ«ì í†µì¼ (unified_statistics.json)
  9. Related Work ë¹„êµ
  10. ì•½ì–´ ì •ì˜

  ëª¨ë“  í•„ìš”í•œ ì •ë³´ê°€ ìƒì„±ëœ ë¬¸ì„œì— ì¤€ë¹„ë˜ì–´ ìˆìŠµë‹ˆë‹¤.